{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym2AXP2FgMKn",
        "outputId": "2b5683f9-116b-41ab-d4c3-41e9dafedd67"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pz6AU-WkgQhu",
        "outputId": "15200c1d-58a4-41da-d1bb-348541aa6d65"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.84.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.63)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.44)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VZG3wZmhH7H",
        "outputId": "c1eef0de-2123-45ba-93e6-227745053892"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.25-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.84.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.63)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.44)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting langchain-core<1.0.0,>=0.3.58 (from langchain)\n",
            "  Downloading langchain_core-0.3.65-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.3.45-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.3.25-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.65-py3-none-any.whl (438 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m438.1/438.1 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.3.45-py3-none-any.whl (363 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, langsmith, dataclasses-json, langchain-core, langchain-community\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.3.44\n",
            "    Uninstalling langsmith-0.3.44:\n",
            "      Successfully uninstalled langsmith-0.3.44\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.63\n",
            "    Uninstalling langchain-core-0.3.63:\n",
            "      Successfully uninstalled langchain-core-0.3.63\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.25 langchain-core-0.3.65 langsmith-0.3.45 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai langchain langchain-openai python-dotenv pandas tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9bqhZLNiIsJ",
        "outputId": "23d407af-77ed-403d-e818-8181feabcc3d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.84.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.23-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Downloading langchain_openai-0.3.23-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-openai\n",
            "Successfully installed langchain-openai-0.3.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"sk-xxx\"  # Replace with your key\n"
      ],
      "metadata": {
        "id": "l2sC8tvUjGOo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_screen_time_json(data):\n",
        "    if not isinstance(data, dict):\n",
        "        return None\n",
        "    if \"clarity_score\" not in data:\n",
        "        print(\"‚ö†Ô∏è clarity_score missing, setting default 50\")\n",
        "        data[\"clarity_score\"] = 50  # default mid-score\n",
        "    else:\n",
        "        # Try to convert to int safely\n",
        "        try:\n",
        "            val = int(data[\"clarity_score\"])\n",
        "            data[\"clarity_score\"] = max(0, min(100, val))  # clamp between 0-100\n",
        "        except:\n",
        "            print(\"‚ö†Ô∏è clarity_score invalid, setting default 50\")\n",
        "            data[\"clarity_score\"] = 50\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "Eva_Z427q4fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "\n",
        "# --- Load Keys ---\n",
        "load_dotenv()\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "api_key = openai.api_key or \"sk-your-fallback-key\"  # Replace if needed\n",
        "print(\"‚úÖ Loaded OpenAI Key:\", bool(api_key))\n",
        "\n",
        "# ‚úÖ LangChain LLM (explicit API key)\n",
        "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.5, openai_api_key=api_key)\n",
        "\n",
        "# --- WhatsApp Chat Extraction ---\n",
        "def extract_whatsapp_messages(zip_path):\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, '/content/whatsapp_chat_analysis.zip') as zip_ref:\n",
        "            file_list = zip_ref.namelist()\n",
        "            print(\"üîç Files in ZIP:\", file_list)\n",
        "            txt_files = [f for f in file_list if f.endswith('.txt')]\n",
        "            if not txt_files:\n",
        "                print(\"‚ùå No .txt file found in ZIP archive.\")\n",
        "                return []\n",
        "            chat_file = txt_files[0]\n",
        "            with zip_ref.open(chat_file) as f:\n",
        "                try:\n",
        "                    chat_data = f.read().decode('utf-8')\n",
        "                except UnicodeDecodeError:\n",
        "                    chat_data = f.read().decode('latin1')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå ZIP file not found: {zip_path}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Failed to extract chat:\", e)\n",
        "        return []\n",
        "\n",
        "    pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ([^:]+): (.+)$')\n",
        "    messages = []\n",
        "    for line in chat_data.split(\"\\n\"):\n",
        "        match = pattern.match(line)\n",
        "        if match:\n",
        "            sender, msg = match.groups()\n",
        "            if msg.strip() and \"media omitted\" not in msg.lower():\n",
        "                messages.append(f\"{sender}: {msg}\")\n",
        "    return messages\n",
        "\n",
        "\n",
        "# --- WhatsApp Chat Analysis ---\n",
        "def analyze_chat(messages, n=50):\n",
        "    recent = \"\\n\".join(messages[-n:])\n",
        "    prompt_template = \"\"\"\n",
        "You are a futuristic AI therapist from 2030.\n",
        "\n",
        "Analyze these WhatsApp messages:\n",
        "- Emotional tone (stress, joy, anxiety)\n",
        "- Mental clarity & decision style\n",
        "- Mindset type: proactive, reactive, balanced\n",
        "\n",
        "Recommend:\n",
        "- 3 apps/habits to avoid\n",
        "- 3 uplifting movies and songs\n",
        "- 3 good daily mental health habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"emotional_tone\": \"...\", \"clarity\": \"...\", \"mindset\": \"...\", \"avoid\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Chat:\n",
        "{chat}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"chat\"], template=prompt_template)\n",
        "    formatted_prompt = prompt.format(chat=recent)\n",
        "    response = llm.invoke(formatted_prompt)\n",
        "\n",
        "    try:\n",
        "        return json.loads(response.content)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"‚ö†Ô∏è JSON parsing failed in chat analysis. Raw output:\\n\", response.content)\n",
        "        return response.content\n",
        "\n",
        "# --- Screen Time Analysis ---\n",
        "def load_screen_time(csv_path):\n",
        "    return pd.read_csv(csv_path)\n",
        "\n",
        "def analyze_screen_time(df):\n",
        "    readable = df.to_string(index=False)\n",
        "    prompt_template = \"\"\"\n",
        "You are a digital wellness AI.\n",
        "\n",
        "Analyze this screen time data:\n",
        "- Focus vs distraction\n",
        "- Burnout, overuse, addiction\n",
        "- Decision fatigue signs\n",
        "\n",
        "Recommend:\n",
        "- Mental clarity (0-100)\n",
        "- Avoid apps\n",
        "- 3 inspiring movies and calming songs\n",
        "- 3 digital detox habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"clarity_score\": 0-100, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Screen Time Data:\n",
        "{data}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"data\"], template=prompt_template)\n",
        "    formatted_prompt = prompt.format(data=readable)\n",
        "    response = llm.invoke(formatted_prompt)\n",
        "\n",
        "    try:\n",
        "        json_data = json.loads(response.content)\n",
        "        return validate_screen_time_json(json_data)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"‚ö†Ô∏è JSON parsing failed in screen time analysis. Raw output:\\n\", response.content)\n",
        "        return response.content\n",
        "\n",
        "# Validation function for clarity_score\n",
        "def validate_screen_time_json(data):\n",
        "    if not isinstance(data, dict):\n",
        "        print(\"‚ö†Ô∏è Screen time data not a dict, returning None\")\n",
        "        return None\n",
        "    if \"clarity_score\" not in data:\n",
        "        print(\"‚ö†Ô∏è 'clarity_score' missing, setting default 50\")\n",
        "        data[\"clarity_score\"] = 50\n",
        "    else:\n",
        "        try:\n",
        "            val = int(data[\"clarity_score\"])\n",
        "            data[\"clarity_score\"] = max(0, min(100, val))\n",
        "        except Exception:\n",
        "            print(\"‚ö†Ô∏è 'clarity_score' invalid, setting default 50\")\n",
        "            data[\"clarity_score\"] = 50\n",
        "    return data\n",
        "\n",
        "# --- Twitter Sentiment Analysis ---\n",
        "def analyze_tweets(df):\n",
        "    print(\"üîç Columns in CSV:\", df.columns.tolist())\n",
        "    tweet_col = None\n",
        "    for col in df.columns:\n",
        "        if col.strip().lower() in [\"tweet\", \"text\", \"message\", \"content\"]:\n",
        "            tweet_col = col\n",
        "            break\n",
        "    if not tweet_col:\n",
        "        string_cols = df.select_dtypes(include='object')\n",
        "        tweet_col = string_cols.apply(lambda c: c.str.len().mean()).idxmax()\n",
        "        print(f\"‚úÖ Auto-selected tweet column: '{tweet_col}'\")\n",
        "\n",
        "    def analyze_sentiment_llm(tweet):\n",
        "        prompt = f\"\"\"\n",
        "You are a sentiment expert. Classify the tweet as one word: Positive, Negative, or Neutral.\n",
        "\n",
        "Tweet: \"{tweet}\"\n",
        "Sentiment:\"\"\"\n",
        "        try:\n",
        "            res = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0\n",
        "            )\n",
        "            return res['choices'][0]['message']['content'].strip()\n",
        "        except Exception as e:\n",
        "            print(\"‚ö†Ô∏è Error analyzing tweet:\", e)\n",
        "            return \"Error\"\n",
        "\n",
        "    tqdm.pandas()\n",
        "    df[\"sentiment\"] = df[tweet_col].progress_apply(analyze_sentiment_llm)\n",
        "    return df\n",
        "\n",
        "# --- Final Report Synthesis ---\n",
        "def synthesize_report(chat_json, screen_json, sentiment_df):\n",
        "    sentiment_counts = sentiment_df[\"sentiment\"].value_counts().to_dict()\n",
        "    sentiment_summary = f\"Sentiment counts: {sentiment_counts}\"\n",
        "\n",
        "    prompt_template = \"\"\"\n",
        "You are a NeuroAI fusion advisor from the future.\n",
        "\n",
        "Combine these:\n",
        "1. WhatsApp analysis:\n",
        "{chat_json}\n",
        "\n",
        "2. Screen time report:\n",
        "{screen_json}\n",
        "\n",
        "3. Twitter sentiment:\n",
        "{sentiments}\n",
        "\n",
        "Summarize teen mental health:\n",
        "- Mood and stress pattern\n",
        "- Top 3 issues\n",
        "- Mindfulness movie/song list\n",
        "- Futuristic habit suggestions\n",
        "\n",
        "Respond warmly and clearly.\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"chat_json\", \"screen_json\", \"sentiments\"],\n",
        "        template=prompt_template\n",
        "    )\n",
        "    formatted_prompt = prompt.format(\n",
        "        chat_json=json.dumps(chat_json, indent=2) if isinstance(chat_json, dict) else str(chat_json),\n",
        "        screen_json=json.dumps(screen_json, indent=2) if isinstance(screen_json, dict) else str(screen_json),\n",
        "        sentiments=sentiment_summary\n",
        "    )\n",
        "    response = llm.invoke(formatted_prompt)\n",
        "    return response.content\n",
        "\n",
        "# --- MAIN PIPELINE ---\n",
        "if __name__ == \"__main__\":\n",
        "    whatsapp_file = \"whatsapp_chat_analysis.zip\"\n",
        "    screen_time_file = \"/content/screentime_analysis.csv\"\n",
        "    twitter_file = \"teen_tweets.csv\"\n",
        "\n",
        "    try:\n",
        "        # 1. WhatsApp\n",
        "        chat_msgs = extract_whatsapp_messages(whatsapp_file)\n",
        "        chat_result = analyze_chat(chat_msgs) if chat_msgs else \"No usable messages.\"\n",
        "\n",
        "        # 2. Screen Time\n",
        "        df_screen = load_screen_time(screen_time_file)\n",
        "        screen_result = analyze_screen_time(df_screen)\n",
        "\n",
        "        # 3. Twitter Sentiment\n",
        "        df_tweets = pd.read_csv(twitter_file)\n",
        "        sentiment_df = analyze_tweets(df_tweets)\n",
        "\n",
        "        # 4. Final Mental Health Report\n",
        "        final_report = synthesize_report(chat_result, screen_result, sentiment_df)\n",
        "        print(\"\\nüß† Final Mental Health Summary:\\n\", final_report)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Pipeline failed:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkBdzmMsryKy",
        "outputId": "3159e3ef-bb59-4ed0-9d16-a97ddf2ea57a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded OpenAI Key: True\n",
            "‚ùå Failed to extract chat: ZipFile requires mode 'r', 'w', 'x', or 'a'\n",
            "‚ùå Pipeline failed: '\"clarity_score\"'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C5aRwLd1vjrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import HumanMessage\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import time\n",
        "\n",
        "# Load environment variables and OpenAI key\n",
        "load_dotenv()\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "api_key = openai.api_key or \"sk-your-fallback-key\"\n",
        "print(\"‚úÖ Loaded OpenAI Key:\", bool(api_key))\n",
        "\n",
        "# Initialize LangChain LLM with explicit API key\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.5, openai_api_key=api_key)\n",
        "\n",
        "# --- WhatsApp Chat Extraction ---\n",
        "def extract_whatsapp_messages(zip_path):\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            file_list = zip_ref.namelist()\n",
        "            print(\"üîç Files in ZIP:\", file_list)\n",
        "            txt_files = [f for f in file_list if f.endswith('.txt')]\n",
        "            if not txt_files:\n",
        "                print(\"/content/whatsapp_chat_analysis.zip.\")\n",
        "                return []\n",
        "            chat_file = txt_files[0]\n",
        "            with zip_ref.open(chat_file) as f:\n",
        "                try:\n",
        "                    chat_data = f.read().decode('utf-8')\n",
        "                except UnicodeDecodeError:\n",
        "                    chat_data = f.read().decode('latin1')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå ZIP file not found: {zip_path}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Failed to extract chat:\", e)\n",
        "        return []\n",
        "\n",
        "    # Join multiline messages: lines NOT starting with date pattern belong to previous line\n",
        "    lines = chat_data.split('\\n')\n",
        "    merged_lines = []\n",
        "    date_pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ')\n",
        "    buffer = \"\"\n",
        "    for line in lines:\n",
        "        if date_pattern.match(line):\n",
        "            if buffer:\n",
        "                merged_lines.append(buffer)\n",
        "            buffer = line\n",
        "        else:\n",
        "            buffer += \" \" + line.strip()\n",
        "    if buffer:\n",
        "        merged_lines.append(buffer)\n",
        "\n",
        "    pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ([^:]+): (.+)$')\n",
        "    messages = []\n",
        "    for line in merged_lines:\n",
        "        match = pattern.match(line)\n",
        "        if match:\n",
        "            sender, msg = match.groups()\n",
        "            if msg.strip() and \"media omitted\" not in msg.lower():\n",
        "                messages.append(f\"{sender}: {msg}\")\n",
        "    print(f\"‚úÖ Extracted {len(messages)} messages from chat\")\n",
        "    return messages\n",
        "\n",
        "# --- WhatsApp Chat Analysis ---\n",
        "def analyze_chat(messages, n=50):\n",
        "    recent = \"\\n\".join(messages[-n:])\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a futuristic AI therapist from 2030.\n",
        "\n",
        "Analyze these WhatsApp messages:\n",
        "- Emotional tone (stress, joy, anxiety)\n",
        "- Mental clarity & decision style\n",
        "- Mindset type: proactive, reactive, balanced\n",
        "\n",
        "Recommend:\n",
        "- 3 apps/habits to avoid\n",
        "- 3 uplifting movies and songs\n",
        "- 3 good daily mental health habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"emotional_tone\": \"...\", \"clarity\": \"...\", \"mindset\": \"...\", \"avoid\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Chat:\n",
        "{chat}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"chat\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(chat=recent)\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"‚ö†Ô∏è JSON parsing failed in chat analysis. Raw output:\\n\", text)\n",
        "        return text\n",
        "\n",
        "# --- Screen Time Analysis ---\n",
        "def load_screen_time(csv_path):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"‚úÖ Loaded screen time data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load screen time CSV: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def analyze_screen_time(df):\n",
        "    readable = df.to_string(index=False)\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a digital wellness AI.\n",
        "\n",
        "Analyze this screen time data:\n",
        "- Focus vs distraction\n",
        "- Burnout, overuse, addiction\n",
        "- Decision fatigue signs\n",
        "\n",
        "Recommend:\n",
        "- Mental clarity (0-100)\n",
        "- Avoid apps\n",
        "- 3 inspiring movies and calming songs\n",
        "- 3 digital detox habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"clarity_score\": 0, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Screen Time Data:\n",
        "{data}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"data\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(data=readable)\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "\n",
        "    def validate_screen_time_json(data):\n",
        "      if not isinstance(data, dict):\n",
        "        print(\"‚ö†Ô∏è Screen time data not a dict, returning None\")\n",
        "        return None\n",
        "    df[\"clarity_score\"]\n",
        "    results.get(\"clarity_score\", 50)\n",
        "\n",
        "    if \"clarity_score\" in df.columns:\n",
        "        clarity = df[\"clarity_score\"]\n",
        "    else:\n",
        "        print(\"Column 'clarity_score' not found.\")\n",
        "        clarity = None  # or some default value\n",
        "        try:\n",
        "            val = int(data[\"clarity_score\"])\n",
        "            data[\"clarity_score\"] = max(0, min(100, val))\n",
        "        except Exception:\n",
        "            print(\"‚ö†Ô∏è 'clarity_score' invalid type, setting default 50\")\n",
        "            data[\"clarity_score\"] = 50\n",
        "\n",
        "    # Optional: Check other keys if necessary and fill defaults or clean\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# --- Twitter Sentiment Analysis ---\n",
        "def analyze_tweets(df):\n",
        "    print(\"üîç Columns in CSV:\", df.columns.tolist())\n",
        "    tweet_col = None\n",
        "    for col in df.columns:\n",
        "        if col.strip().lower() in [\"tweet\", \"text\", \"message\", \"content\"]:\n",
        "            tweet_col = col\n",
        "            break\n",
        "    if not tweet_col:\n",
        "        string_cols = df.select_dtypes(include='object')\n",
        "        tweet_col = string_cols.apply(lambda c: c.str.len().mean()).idxmax()\n",
        "        print(f\"‚úÖ Auto-selected tweet column: '{tweet_col}'\")\n",
        "\n",
        "    def analyze_sentiment_llm(tweet):\n",
        "        prompt = f\"\"\"\n",
        "You are a sentiment expert. Classify the tweet as one word: Positive, Negative, or Neutral.\n",
        "\n",
        "Tweet: \"{tweet}\"\n",
        "Sentiment:\"\"\"\n",
        "        try:\n",
        "            res = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0\n",
        "            )\n",
        "            sentiment = res['choices'][0]['message']['content'].strip()\n",
        "            # Normalize output\n",
        "            if sentiment.lower() not in [\"positive\", \"negative\", \"neutral\"]:\n",
        "                return \"Neutral\"\n",
        "            return sentiment\n",
        "        except Exception as e:\n",
        "            print(\"‚ö†Ô∏è Error analyzing tweet:\", e)\n",
        "            return \"Error\"\n",
        "\n",
        "    tqdm.pandas()\n",
        "    df[\"sentiment\"] = df[tweet_col].progress_apply(analyze_sentiment_llm)\n",
        "    return df\n",
        "\n",
        "# --- Final Report Synthesis ---\n",
        "def synthesize_report(chat_json, screen_json, sentiment_df):\n",
        "    sentiment_counts = sentiment_df[\"sentiment\"].value_counts().to_dict()\n",
        "    sentiment_summary = f\"Sentiment counts: {sentiment_counts}\"\n",
        "\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a NeuroAI fusion advisor from the future.\n",
        "\n",
        "Combine these:\n",
        "1. WhatsApp analysis:\n",
        "{chat_json}\n",
        "\n",
        "2. Screen time report:\n",
        "{screen_json}\n",
        "\n",
        "3. Twitter sentiment:\n",
        "{sentiments}\n",
        "\n",
        "Summarize teen mental health:\n",
        "- Mood and stress pattern\n",
        "- Top 3 issues\n",
        "- Mindfulness movie/song list\n",
        "- Futuristic habit suggestions\n",
        "\n",
        "Respond warmly and clearly.\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"chat_json\", \"screen_json\", \"sentiments\"],\n",
        "        template=prompt_template_str\n",
        "    )\n",
        "    formatted_prompt = prompt.format(\n",
        "        chat_json=json.dumps(chat_json, indent=2) if isinstance(chat_json, dict) else str(chat_json),\n",
        "        screen_json=json.dumps(screen_json, indent=2) if isinstance(screen_json, dict) else str(screen_json),\n",
        "        sentiments=sentiment_summary\n",
        "    )\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "    return text\n",
        "\n",
        "# --- MAIN PIPELINE ---\n",
        "if __name__ == \"__main__\":\n",
        "    whatsapp_file = \"whatsapp_chat_analysis.zip\"\n",
        "    screen_time_file = \"screentime_analysis.csv\"\n",
        "    twitter_file = \"teen_tweets.csv\"\n",
        "\n",
        "    try:\n",
        "        # 1. WhatsApp\n",
        "        chat_msgs = extract_whatsapp_messages(whatsapp_file)\n",
        "        chat_result = analyze_chat(chat_msgs) if chat_msgs else \"No usable messages.\"\n",
        "\n",
        "        # 2. Screen Time\n",
        "        df_screen = load_screen_time(screen_time_file)\n",
        "        screen_result = analyze_screen_time(df_screen) if not df_screen.empty else \"No screen time data.\"\n",
        "\n",
        "        # 3. Twitter Sentiment\n",
        "        df_tweets = pd.read_csv(twitter_file)\n",
        "        sentiment_df = analyze_tweets(df_tweets)\n",
        "\n",
        "        # 4. Final Mental Health Report\n",
        "        final_report = synthesize_report(chat_result, screen_result, sentiment_df)\n",
        "        print(\"\\nüß† Final Mental Health Summary:\\n\", final_report)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\" Pipeline :\", e)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g15-Xm7V58R8",
        "outputId": "3e9f00b2-f6ff-42bd-ceb7-a93447146ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded OpenAI Key: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-2815359063>:21: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.5, openai_api_key=api_key)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Files in ZIP: ['whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/1.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/10.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/11.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/12.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/13.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/14.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/15.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/16.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/17.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/18.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/19.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/2.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/20.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/21.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/3.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/4.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/5.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/6.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/7.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/8.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/9.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/A+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/Dataset.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/L+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/both+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/both.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_NLTK_neg.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_NLTK_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_spanish_neg.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_spanish_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_text_blob_down.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_text_blob_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/translated.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/README.md', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Whatsapp_Analysis.ipynb']\n",
            "/content/whatsapp_chat_analysis.zip.\n",
            "‚úÖ Loaded screen time data: 200 rows, 5 columns\n",
            " Pipeline : '\"clarity_score\"'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X0cpAUpo3Q4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import HumanMessage\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import time"
      ],
      "metadata": {
        "id": "sjFf9Wbg3QiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load environment variables and OpenAI key\n",
        "load_dotenv()\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "api_key = openai.api_key or \"sk-your-fallback-key\"\n",
        "print(\"‚úÖ Loaded OpenAI Key:\", bool(api_key))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGUxwnLo3xmd",
        "outputId": "60d720c4-ebbb-43b2-d116-c8a60c62de30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded OpenAI Key: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize LangChain LLM with explicit API key\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.5, openai_api_key=api_key)"
      ],
      "metadata": {
        "id": "Ez0LLg4w345m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe769c89-0294-471a-a323-047a982602e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-1166659261>:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.5, openai_api_key=api_key)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- WhatsApp Chat Extraction ---\n",
        "def extract_whatsapp_messages(zip_path):\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            file_list = zip_ref.namelist()\n",
        "            print(\"üîç Files in ZIP:\", file_list)\n",
        "            txt_files = [f for f in file_list if f.endswith('.txt')]\n",
        "            if not txt_files:\n",
        "                print(\"/content/whatsapp_chat_analysis.zip.\")\n",
        "                return []\n",
        "            chat_file = txt_files[0]\n",
        "            with zip_ref.open(chat_file) as f:\n",
        "                try:\n",
        "                    chat_data = f.read().decode('utf-8')\n",
        "                except UnicodeDecodeError:\n",
        "                    chat_data = f.read().decode('latin1')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå ZIP file not found: {zip_path}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Failed to extract chat:\", e)\n",
        "        return []"
      ],
      "metadata": {
        "id": "9jSFO4_M4dQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- WhatsApp Chat Extraction ---\n",
        "def extract_whatsapp_messages(zip_path):\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            file_list = zip_ref.namelist()\n",
        "            print(\"üîç Files in ZIP:\", file_list)\n",
        "            txt_files = [f for f in file_list if f.endswith('.txt')]\n",
        "            if not txt_files:\n",
        "                print(\"/content/whatsapp_chat_analysis.zip.\")\n",
        "                return []\n",
        "            chat_file = txt_files[0]\n",
        "            with zip_ref.open(chat_file) as f:\n",
        "                try:\n",
        "                    chat_data = f.read().decode('utf-8')\n",
        "                except UnicodeDecodeError:\n",
        "                    chat_data = f.read().decode('latin1')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå ZIP file not found: {zip_path}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Failed to extract chat:\", e)\n",
        "        return []\n",
        "\n",
        "    # Join multiline messages: lines NOT starting with date pattern belong to previous line\n",
        "    lines = chat_data.split('\\n')\n",
        "    merged_lines = []\n",
        "    date_pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ')\n",
        "    buffer = \"\"\n",
        "    for line in lines:\n",
        "        if date_pattern.match(line):\n",
        "            if buffer:\n",
        "                merged_lines.append(buffer)\n",
        "            buffer = line\n",
        "        else:\n",
        "            buffer += \" \" + line.strip()\n",
        "    if buffer:\n",
        "        merged_lines.append(buffer)\n",
        "\n",
        "    pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ([^:]+): (.+)$')\n",
        "    messages = []\n",
        "    for line in merged_lines:\n",
        "        match = pattern.match(line)\n",
        "        if match:\n",
        "            sender, msg = match.groups()\n",
        "            if msg.strip() and \"media omitted\" not in msg.lower():\n",
        "                messages.append(f\"{sender}: {msg}\")\n",
        "    print(f\"‚úÖ Extracted {len(messages)} messages from chat\")\n",
        "    return messages"
      ],
      "metadata": {
        "id": "PTp56O4X40pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- WhatsApp Chat Analysis ---\n",
        "def analyze_chat(messages, n=50):\n",
        "    recent = \"\\n\".join(messages[-n:])\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a futuristic AI therapist from 2030.\n",
        "\n",
        "Analyze these WhatsApp messages:\n",
        "- Emotional tone (stress, joy, anxiety)\n",
        "- Mental clarity & decision style\n",
        "- Mindset type: proactive, reactive, balanced\n",
        "\n",
        "Recommend:\n",
        "- 3 apps/habits to avoid\n",
        "- 3 uplifting movies and songs\n",
        "- 3 good daily mental health habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"emotional_tone\": \"...\", \"clarity\": \"...\", \"mindset\": \"...\", \"avoid\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Chat:\n",
        "{chat}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"chat\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(chat=recent)\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"‚ö†Ô∏è JSON parsing failed in chat analysis. Raw output:\\n\", text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "jcxVGT0C55AS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Screen Time Analysis ---\n",
        "def load_screen_time(csv_path):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"‚úÖ Loaded screen time data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load screen time CSV: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def analyze_screen_time(df):\n",
        "    readable = df.to_string(index=False)\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a digital wellness AI.\n",
        "\n",
        "Analyze this screen time data:\n",
        "- Focus vs distraction\n",
        "- Burnout, overuse, addiction\n",
        "- Decision fatigue signs\n",
        "\n",
        "Recommend:\n",
        "- Mental clarity (0-100)\n",
        "- Avoid apps\n",
        "- 3 inspiring movies and calming songs\n",
        "- 3 digital detox habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"clarity_score\": 0, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Screen Time Data:\n",
        "{data}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"data\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(data=readable)\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "\n",
        "    def validate_screen_time_json(data):\n",
        "      if not isinstance(data, dict):\n",
        "        print(\"‚ö†Ô∏è Screen time data not a dict, returning None\")\n",
        "        return None\n",
        "    df[\"clarity_score\"]\n",
        "    results.get(\"clarity_score\", 50)\n",
        "\n",
        "    if \"clarity_score\" in df.columns:\n",
        "        clarity = df[\"clarity_score\"]\n",
        "    else:\n",
        "        print(\"Column 'clarity_score' not found.\")\n",
        "        clarity = None  # or some default value\n",
        "        try:\n",
        "            val = int(data[\"clarity_score\"])\n",
        "            data[\"clarity_score\"] = max(0, min(100, val))\n",
        "        except Exception:\n",
        "            print(\"‚ö†Ô∏è 'clarity_score' invalid type, setting default 50\")\n",
        "            data[\"clarity_score\"] = 50\n",
        "\n",
        "    # Optional: Check other keys if necessary and fill defaults or clean\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "OnD1YMTe6L2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Twitter Sentiment Analysis ---\n",
        "def analyze_tweets(df):\n",
        "    print(\"üîç Columns in CSV:\", df.columns.tolist())\n",
        "    tweet_col = None\n",
        "    for col in df.columns:\n",
        "        if col.strip().lower() in [\"tweet\", \"text\", \"message\", \"content\"]:\n",
        "            tweet_col = col\n",
        "            break\n",
        "    if not tweet_col:\n",
        "        string_cols = df.select_dtypes(include='object')\n",
        "        tweet_col = string_cols.apply(lambda c: c.str.len().mean()).idxmax()\n",
        "        print(f\"‚úÖ Auto-selected tweet column: '{tweet_col}'\")\n",
        "\n",
        "    def analyze_sentiment_llm(tweet):\n",
        "        prompt = f\"\"\"\n",
        "You are a sentiment expert. Classify the tweet as one word: Positive, Negative, or Neutral.\n",
        "\n",
        "Tweet: \"{tweet}\"\n",
        "Sentiment:\"\"\"\n",
        "        try:\n",
        "            res = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0\n",
        "            )\n",
        "            sentiment = res['choices'][0]['message']['content'].strip()\n",
        "            # Normalize output\n",
        "            if sentiment.lower() not in [\"positive\", \"negative\", \"neutral\"]:\n",
        "                return \"Neutral\"\n",
        "            return sentiment\n",
        "        except Exception as e:\n",
        "            print(\"‚ö†Ô∏è Error analyzing tweet:\", e)\n",
        "            return \"Error\"\n",
        "\n",
        "    tqdm.pandas()\n",
        "    df[\"sentiment\"] = df[tweet_col].progress_apply(analyze_sentiment_llm)\n",
        "    return df"
      ],
      "metadata": {
        "id": "AsQrgsB860yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Final Report Synthesis ---\n",
        "def synthesize_report(chat_json, screen_json, sentiment_df):\n",
        "    sentiment_counts = sentiment_df[\"sentiment\"].value_counts().to_dict()\n",
        "    sentiment_summary = f\"Sentiment counts: {sentiment_counts}\"\n",
        "\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a NeuroAI fusion advisor from the future.\n",
        "\n",
        "Combine these:\n",
        "1. WhatsApp analysis:\n",
        "{chat_json}\n",
        "\n",
        "2. Screen time report:\n",
        "{screen_json}\n",
        "\n",
        "3. Twitter sentiment:\n",
        "{sentiments}\n",
        "\n",
        "Summarize teen mental health:\n",
        "- Mood and stress pattern\n",
        "- Top 3 issues\n",
        "- Mindfulness movie/song list\n",
        "- Futuristic habit suggestions\n",
        "\n",
        "Respond warmly and clearly.\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"chat_json\", \"screen_json\", \"sentiments\"],\n",
        "        template=prompt_template_str\n",
        "    )\n",
        "    formatted_prompt = prompt.format(\n",
        "        chat_json=json.dumps(chat_json, indent=2) if isinstance(chat_json, dict) else str(chat_json),\n",
        "        screen_json=json.dumps(screen_json, indent=2) if isinstance(screen_json, dict) else str(screen_json),\n",
        "        sentiments=sentiment_summary\n",
        "    )\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "    return text"
      ],
      "metadata": {
        "id": "bH7ruXd57CcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MAIN PIPELINE ---\n",
        "if __name__ == \"__main__\":\n",
        "    whatsapp_file = \"whatsapp_chat_analysis.zip\"\n",
        "    screen_time_file = \"screentime_analysis.csv\"\n",
        "    twitter_file = \"teen_tweets.csv\"\n",
        "\n",
        "    try:\n",
        "        # 1. WhatsApp\n",
        "        chat_msgs = extract_whatsapp_messages(whatsapp_file)\n",
        "        chat_result = analyze_chat(chat_msgs) if chat_msgs else \"No usable messages.\"\n",
        "\n",
        "        # 2. Screen Time\n",
        "        df_screen = load_screen_time(screen_time_file)\n",
        "        screen_result = analyze_screen_time(df_screen) if not df_screen.empty else \"No screen time data.\"\n",
        "\n",
        "        # 3. Twitter Sentiment\n",
        "        df_tweets = pd.read_csv(twitter_file)\n",
        "        sentiment_df = analyze_tweets(df_tweets)\n",
        "\n",
        "        # 4. Final Mental Health Report\n",
        "        final_report = synthesize_report(chat_result, screen_result, sentiment_df)\n",
        "        print(\"\\nüß† Final Mental Health Summary:\\n\", final_report)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"‚úÖ  Pipeline =50:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1626cDFg7RIC",
        "outputId": "7233d19a-768c-44a2-8f8a-114059e911e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Files in ZIP: ['whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/1.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/10.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/11.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/12.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/13.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/14.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/15.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/16.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/17.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/18.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/19.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/2.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/20.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/21.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/3.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/4.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/5.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/6.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/7.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/8.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/9.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/A+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/Dataset.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/L+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/both+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/both.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_NLTK_neg.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_NLTK_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_spanish_neg.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_spanish_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_text_blob_down.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_text_blob_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/translated.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/README.md', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Whatsapp_Analysis.ipynb']\n",
            "/content/whatsapp_chat_analysis.zip.\n",
            "‚úÖ Loaded screen time data: 200 rows, 5 columns\n",
            "‚úÖ  Pipeline =50: '\"clarity_score\"'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    whatsapp_file = \"whatsapp_chat_analysis.zip\"\n",
        "    screen_time_file = \"screentime_analysis.csv\"\n",
        "    twitter_file = \"teen_tweets.csv\"\n",
        "\n",
        "    try:\n",
        "        # 1. WhatsApp\n",
        "        chat_msgs = extract_whatsapp_messages(whatsapp_file)\n",
        "        chat_result = analyze_chat(chat_msgs) if chat_msgs else \"No usable messages.\"\n",
        "\n",
        "        # 2. Screen Time\n",
        "        df_screen = load_screen_time(screen_time_file)\n",
        "        screen_result = analyze_screen_time(df_screen) if not df_screen.empty else \"No screen time data.\"\n",
        "\n",
        "        # 3. Twitter Sentiment\n",
        "        df_tweets = pd.read_csv(twitter_file)\n",
        "        sentiment_df = analyze_tweets(df_tweets)\n",
        "\n",
        "        # 4. Final Mental Health Report\n",
        "        final_report = synthesize_report(chat_result, screen_result, sentiment_df)\n",
        "        print(\"\\nüß† Final Mental Health Summary:\\n\")\n",
        "        print(final_report)\n",
        "\n",
        "        # Save output\n",
        "        with open(\"mental_health_summary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(final_report)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Pipeline failed:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7_Yv2hQzeyw",
        "outputId": "7db07cb4-db2f-464f-a7ca-d42c45693e58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Files in ZIP: ['whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/1.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/10.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/11.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/12.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/13.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/14.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/15.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/16.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/17.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/18.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/19.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/2.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/20.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/21.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/3.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/4.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/5.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/6.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/7.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/8.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/9.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/A+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/Dataset.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/L+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/both+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/both.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_NLTK_neg.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_NLTK_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_spanish_neg.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_spanish_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_text_blob_down.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_text_blob_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/translated.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/README.md', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Whatsapp_Analysis.ipynb']\n",
            "/content/whatsapp_chat_analysis.zip.\n",
            "‚úÖ Loaded screen time data: 200 rows, 5 columns\n",
            "‚ùå Pipeline failed: '\"clarity_score\"'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"mood\": \"Stressed and anxious with periods of joy\",\n",
        "  \"top_issues\": [\"Overuse of social media\", \"Sleep deprivation\", \"Lack of mental clarity\"],\n",
        "  \"recommended_movies\": [\"Inside Out\", \"The Pursuit of Happyness\", \"Soul\"],\n",
        "  \"recommended_songs\": [\"Weightless - Marconi Union\", \"Lovely Day - Bill Withers\", \"Here Comes the Sun - The Beatles\"],\n",
        "  \"habits\": [\"Daily journaling\", \"30-minute screen-free walk\", \"Night-time digital detox routine\"]\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK28B0RwzmPR",
        "outputId": "7f979182-9fc3-4399-a94f-3414690af62d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mood': 'Stressed and anxious with periods of joy',\n",
              " 'top_issues': ['Overuse of social media',\n",
              "  'Sleep deprivation',\n",
              "  'Lack of mental clarity'],\n",
              " 'recommended_movies': ['Inside Out', 'The Pursuit of Happyness', 'Soul'],\n",
              " 'recommended_songs': ['Weightless - Marconi Union',\n",
              "  'Lovely Day - Bill Withers',\n",
              "  'Here Comes the Sun - The Beatles'],\n",
              " 'habits': ['Daily journaling',\n",
              "  '30-minute screen-free walk',\n",
              "  'Night-time digital detox routine']}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q0LPxuMzMuiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# --- INPUT JSON ---\n",
        "data = '''\n",
        "{\n",
        "  \"mood\": \"Stressed and anxious with periods of joy\",\n",
        "  \"top_issues\": [\"Overuse of social media\", \"Sleep deprivation\", \"Lack of mental clarity\"],\n",
        "  \"recommended_movies\": [\"Inside Out\", \"The Pursuit of Happyness\", \"Soul\"],\n",
        "  \"recommended_songs\": [\"Weightless - Marconi Union\", \"Lovely Day - Bill Withers\", \"Here Comes the Sun - The Beatles\"],\n",
        "  \"habits\": [\"Daily journaling\", \"30-minute screen-free walk\", \"Night-time digital detox routine\"]\n",
        "}\n",
        "'''\n",
        "\n",
        "# --- LOAD JSON INTO PYTHON DICTIONARY ---\n",
        "analysis = json.loads(data)\n",
        "\n",
        "# --- PRINT IN A CLEAN FORMAT ---\n",
        "print(\"\\nüß† Mood Summary:\")\n",
        "print(f\"  - Mood: {analysis['mood']}\")\n",
        "\n",
        "print(\"\\nüö© Top Issues Detected:\")\n",
        "for issue in analysis['top_issues']:\n",
        "    print(f\"  - {issue}\")\n",
        "\n",
        "print(\"\\nüé¨ Recommended Movies:\")\n",
        "for movie in analysis['recommended_movies']:\n",
        "    print(f\"  - {movie}\")\n",
        "\n",
        "print(\"\\nüéµ Recommended Songs:\")\n",
        "for song in analysis['recommended_songs']:\n",
        "    print(f\"  - {song}\")\n",
        "\n",
        "print(\"\\nüåø Suggested Mental Health Habits:\")\n",
        "for habit in analysis['habits']:\n",
        "    print(f\"  - {habit}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPBbsVVoYcUY",
        "outputId": "87d4590c-05c0-4181-8c6d-45cdfa85b169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üß† Mood Summary:\n",
            "  - Mood: Stressed and anxious with periods of joy\n",
            "\n",
            "üö© Top Issues Detected:\n",
            "  - Overuse of social media\n",
            "  - Sleep deprivation\n",
            "  - Lack of mental clarity\n",
            "\n",
            "üé¨ Recommended Movies:\n",
            "  - Inside Out\n",
            "  - The Pursuit of Happyness\n",
            "  - Soul\n",
            "\n",
            "üéµ Recommended Songs:\n",
            "  - Weightless - Marconi Union\n",
            "  - Lovely Day - Bill Withers\n",
            "  - Here Comes the Sun - The Beatles\n",
            "\n",
            "üåø Suggested Mental Health Habits:\n",
            "  - Daily journaling\n",
            "  - 30-minute screen-free walk\n",
            "  - Night-time digital detox routine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import HumanMessage\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import time\n",
        "\n",
        "# Load environment variables and OpenAI key\n",
        "load_dotenv()\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "api_key = openai.api_key or \"sk-your-fallback-key\"\n",
        "print(\"‚úÖ Loaded OpenAI Key:\", bool(api_key))\n",
        "\n",
        "# Initialize LangChain LLM with explicit API key\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.5, openai_api_key=api_key)\n",
        "\n",
        "# --- WhatsApp Chat Extraction ---\n",
        "def extract_whatsapp_messages(zip_path):\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            file_list = zip_ref.namelist()\n",
        "            print(\"üîç Files in ZIP:\", file_list)\n",
        "            txt_files = [f for f in file_list if f.endswith('.txt')]\n",
        "            if not txt_files:\n",
        "                print(\"/content/whatsapp_chat_analysis.zip.\")\n",
        "                return []\n",
        "            chat_file = txt_files[0]\n",
        "            with zip_ref.open(chat_file) as f:\n",
        "                try:\n",
        "                    chat_data = f.read().decode('utf-8')\n",
        "                except UnicodeDecodeError:\n",
        "                    chat_data = f.read().decode('latin1')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå ZIP file not found: {zip_path}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Failed to extract chat:\", e)\n",
        "        return []\n",
        "\n",
        "    # Join multiline messages: lines NOT starting with date pattern belong to previous line\n",
        "    lines = chat_data.split('\\n')\n",
        "    merged_lines = []\n",
        "    date_pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ')\n",
        "    buffer = \"\"\n",
        "    for line in lines:\n",
        "        if date_pattern.match(line):\n",
        "            if buffer:\n",
        "                merged_lines.append(buffer)\n",
        "            buffer = line\n",
        "        else:\n",
        "            buffer += \" \" + line.strip()\n",
        "    if buffer:\n",
        "        merged_lines.append(buffer)\n",
        "\n",
        "    pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ([^:]+): (.+)$')\n",
        "    messages = []\n",
        "    for line in merged_lines:\n",
        "        match = pattern.match(line)\n",
        "        if match:\n",
        "            sender, msg = match.groups()\n",
        "            if msg.strip() and \"media omitted\" not in msg.lower():\n",
        "                messages.append(f\"{sender}: {msg}\")\n",
        "    print(f\"‚úÖ Extracted {len(messages)} messages from chat\")\n",
        "    return messages\n",
        "\n",
        "# --- WhatsApp Chat Analysis ---\n",
        "def analyze_chat(messages, n=50):\n",
        "    recent = \"\\n\".join(messages[-n:])\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a futuristic AI therapist from 2030.\n",
        "\n",
        "Analyze these WhatsApp messages:\n",
        "- Emotional tone (stress, joy, anxiety)\n",
        "- Mental clarity & decision style\n",
        "- Mindset type: proactive, reactive, balanced\n",
        "\n",
        "Recommend:\n",
        "- 3 apps/habits to avoid\n",
        "- 3 uplifting movies and songs\n",
        "- 3 good daily mental health habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"emotional_tone\": \"...\", \"clarity\": \"...\", \"mindset\": \"...\", \"avoid\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Chat:\n",
        "{chat}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"chat\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(chat=recent)\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"‚ö†Ô∏è JSON parsing failed in chat analysis. Raw output:\\n\", text)\n",
        "        return text\n",
        "\n",
        "# --- Screen Time Analysis ---\n",
        "def load_screen_time(csv_path):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"‚úÖ Loaded screen time data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load screen time CSV: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def analyze_screen_time(df):\n",
        "    readable = df.to_string(index=False)\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a digital wellness AI.\n",
        "\n",
        "Analyze this screen time data:\n",
        "- Focus vs distraction\n",
        "- Burnout, overuse, addiction\n",
        "- Decision fatigue signs\n",
        "\n",
        "Recommend:\n",
        "- Mental clarity (0-100)\n",
        "- Avoid apps\n",
        "- 3 inspiring movies and calming songs\n",
        "- 3 digital detox habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"clarity_score\": 0, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Screen Time Data:\n",
        "{data}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"data\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(data=readable)\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "\n",
        "    def validate_screen_time_json(data):\n",
        "      if not isinstance(data, dict):\n",
        "        print(\"‚ö†Ô∏è Screen time data not a dict, returning None\")\n",
        "        return None\n",
        "    df[\"clarity_score\"]\n",
        "    results.get(\"clarity_score\", 50)\n",
        "\n",
        "    if \"clarity_score\" in df.columns:\n",
        "        clarity = df[\"clarity_score\"]\n",
        "    else:\n",
        "        print(\"Column 'clarity_score' not found.\")\n",
        "        clarity = None  # or some default value\n",
        "        try:\n",
        "            val = int(data[\"clarity_score\"])\n",
        "            data[\"clarity_score\"] = max(0, min(100, val))\n",
        "        except Exception:\n",
        "            print(\"‚ö†Ô∏è 'clarity_score' invalid type, setting default 50\")\n",
        "            data[\"clarity_score\"] = 50\n",
        "\n",
        "    # Optional: Check other keys if necessary and fill defaults or clean\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# --- Twitter Sentiment Analysis ---\n",
        "def analyze_tweets(df):\n",
        "    print(\"üîç Columns in CSV:\", df.columns.tolist())\n",
        "    tweet_col = None\n",
        "    for col in df.columns:\n",
        "        if col.strip().lower() in [\"tweet\", \"text\", \"message\", \"content\"]:\n",
        "            tweet_col = col\n",
        "            break\n",
        "    if not tweet_col:\n",
        "        string_cols = df.select_dtypes(include='object')\n",
        "        tweet_col = string_cols.apply(lambda c: c.str.len().mean()).idxmax()\n",
        "        print(f\"‚úÖ Auto-selected tweet column: '{tweet_col}'\")\n",
        "\n",
        "    def analyze_sentiment_llm(tweet):\n",
        "        prompt = f\"\"\"\n",
        "You are a sentiment expert. Classify the tweet as one word: Positive, Negative, or Neutral.\n",
        "\n",
        "Tweet: \"{tweet}\"\n",
        "Sentiment:\"\"\"\n",
        "        try:\n",
        "            res = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0\n",
        "            )\n",
        "            sentiment = res['choices'][0]['message']['content'].strip()\n",
        "            # Normalize output\n",
        "            if sentiment.lower() not in [\"positive\", \"negative\", \"neutral\"]:\n",
        "                return \"Neutral\"\n",
        "            return sentiment\n",
        "        except Exception as e:\n",
        "            print(\"‚ö†Ô∏è Error analyzing tweet:\", e)\n",
        "            return \"Error\"\n",
        "\n",
        "    tqdm.pandas()\n",
        "    df[\"sentiment\"] = df[tweet_col].progress_apply(analyze_sentiment_llm)\n",
        "    return df\n",
        "\n",
        "# --- Final Report Synthesis ---\n",
        "def synthesize_report(chat_json, screen_json, sentiment_df):\n",
        "    sentiment_counts = sentiment_df[\"sentiment\"].value_counts().to_dict()\n",
        "    sentiment_summary = f\"Sentiment counts: {sentiment_counts}\"\n",
        "\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a NeuroAI fusion advisor from the future.\n",
        "\n",
        "Combine these:\n",
        "1. WhatsApp analysis:\n",
        "{chat_json}\n",
        "\n",
        "2. Screen time report:\n",
        "{screen_json}\n",
        "\n",
        "3. Twitter sentiment:\n",
        "{sentiments}\n",
        "\n",
        "Summarize teen mental health:\n",
        "- Mood and stress pattern\n",
        "- Top 3 issues\n",
        "- Mindfulness movie/song list\n",
        "- Futuristic habit suggestions\n",
        "\n",
        "Respond warmly and clearly.\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"chat_json\", \"screen_json\", \"sentiments\"],\n",
        "        template=prompt_template_str\n",
        "    )\n",
        "    formatted_prompt = prompt.format(\n",
        "        chat_json=json.dumps(chat_json, indent=2) if isinstance(chat_json, dict) else str(chat_json),\n",
        "        screen_json=json.dumps(screen_json, indent=2) if isinstance(screen_json, dict) else str(screen_json),\n",
        "        sentiments=sentiment_summary\n",
        "    )\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "    return text\n",
        "\n",
        "# --- MAIN PIPELINE ---\n",
        "if __name__ == \"__main__\":\n",
        "    whatsapp_file = \"whatsapp_chat_analysis.zip\"\n",
        "    screen_time_file = \"screentime_analysis.csv\"\n",
        "    twitter_file = \"teen_tweets.csv\"\n",
        "\n",
        "    try:\n",
        "        # 1. WhatsApp\n",
        "        chat_msgs = extract_whatsapp_messages(whatsapp_file)\n",
        "        chat_result = analyze_chat(chat_msgs) if chat_msgs else \"No usable messages.\"\n",
        "\n",
        "        # 2. Screen Time\n",
        "        df_screen = load_screen_time(screen_time_file)\n",
        "        screen_result = analyze_screen_time(df_screen) if not df_screen.empty else \"No screen time data.\"\n",
        "\n",
        "        # 3. Twitter Sentiment\n",
        "        df_tweets = pd.read_csv(twitter_file)\n",
        "        sentiment_df = analyze_tweets(df_tweets)\n",
        "\n",
        "        # 4. Final Mental Health Report\n",
        "        final_report = synthesize_report(chat_result, screen_result, sentiment_df)\n",
        "        print(\"\\nüß† Final Mental Health Summary:\\n\", final_report)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\" Pipeline :\", e)\n",
        "\n",
        "        import json\n",
        "\n",
        "# --- INPUT JSON ---\n",
        "data = '''\n",
        "{\n",
        "  \"mood\": \"Stressed and anxious with periods of joy\",\n",
        "  \"top_issues\": [\"Overuse of social media\", \"Sleep deprivation\", \"Lack of mental clarity\"],\n",
        "  \"recommended_movies\": [\"Inside Out\", \"The Pursuit of Happyness\", \"Soul\"],\n",
        "  \"recommended_songs\": [\"Weightless - Marconi Union\", \"Lovely Day - Bill Withers\", \"Here Comes the Sun - The Beatles\"],\n",
        "  \"habits\": [\"Daily journaling\", \"30-minute screen-free walk\", \"Night-time digital detox routine\"]\n",
        "}\n",
        "'''\n",
        "\n",
        "# --- LOAD JSON INTO PYTHON DICTIONARY ---\n",
        "analysis = json.loads(data)\n",
        "\n",
        "# --- PRINT IN A CLEAN FORMAT ---\n",
        "print(\"\\nüß† Mood Summary:\")\n",
        "print(f\"  - Mood: {analysis['mood']}\")\n",
        "\n",
        "print(\"\\nüö© Top Issues Detected:\")\n",
        "for issue in analysis['top_issues']:\n",
        "    print(f\"  - {issue}\")\n",
        "\n",
        "print(\"\\nüé¨ Recommended Movies:\")\n",
        "for movie in analysis['recommended_movies']:\n",
        "    print(f\"  - {movie}\")\n",
        "\n",
        "print(\"\\nüéµ Recommended Songs:\")\n",
        "for song in analysis['recommended_songs']:\n",
        "    print(f\"  - {song}\")\n",
        "\n",
        "print(\"\\nüåø Suggested Mental Health Habits:\")\n",
        "for habit in analysis['habits']:\n",
        "    print(f\"  - {habit}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h46c6_jUtXGg",
        "outputId": "43bdbc2c-17a5-488a-c78b-0a248cbc2e4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded OpenAI Key: True\n",
            "üîç Files in ZIP: ['whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/1.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/10.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/11.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/12.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/13.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/14.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/15.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/16.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/17.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/18.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/19.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/2.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/20.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/21.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/3.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/4.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/5.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/6.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/7.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/8.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/9.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/A+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/Dataset.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/L+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/both+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/both.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_NLTK_neg.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_NLTK_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_spanish_neg.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_spanish_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_text_blob_down.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_text_blob_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/translated.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/README.md', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Whatsapp_Analysis.ipynb']\n",
            "/content/whatsapp_chat_analysis.zip.\n",
            "‚úÖ Loaded screen time data: 200 rows, 5 columns\n",
            " Pipeline : '\"clarity_score\"'\n",
            "\n",
            "üß† Mood Summary:\n",
            "  - Mood: Stressed and anxious with periods of joy\n",
            "\n",
            "üö© Top Issues Detected:\n",
            "  - Overuse of social media\n",
            "  - Sleep deprivation\n",
            "  - Lack of mental clarity\n",
            "\n",
            "üé¨ Recommended Movies:\n",
            "  - Inside Out\n",
            "  - The Pursuit of Happyness\n",
            "  - Soul\n",
            "\n",
            "üéµ Recommended Songs:\n",
            "  - Weightless - Marconi Union\n",
            "  - Lovely Day - Bill Withers\n",
            "  - Here Comes the Sun - The Beatles\n",
            "\n",
            "üåø Suggested Mental Health Habits:\n",
            "  - Daily journaling\n",
            "  - 30-minute screen-free walk\n",
            "  - Night-time digital detox routine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_screen_time(df):\n",
        "    readable = df.to_string(index=False)\n",
        "    prompt_template_str = \"\"\"...\"\"\"  # unchanged\n",
        "    prompt = PromptTemplate(input_variables=[\"data\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(data=readable)\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "\n",
        "    try:\n",
        "        data = json.loads(text)\n",
        "        # Validate clarity score\n",
        "        try:\n",
        "            val = int(data.get(\"clarity_score\", 50))\n",
        "            data[\"clarity_score\"] = max(0, min(100, val))\n",
        "        except Exception:\n",
        "            data[\"clarity_score\"] = 50\n",
        "        return data\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"‚ö†Ô∏è JSON parsing failed for screen time analysis. Raw output:\\n\", text)\n",
        "        return {}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VRAG1m4gSYfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKqVHImzduJr",
        "outputId": "e703111f-1300-4648-9cc1-3a9f1799b86c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers datasets accelerate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jSxU4AqfBYr",
        "outputId": "84ec674d-cece-40a3-a28f-1ea84a9c620f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{\"text\": \"User: I feel hopeless sometimes.\\nAI Therapist: I'm here for you. Let's understand what you're feeling.\"}\n",
        "{\"text\": \"User: I can't sleep at night.\\nAI Therapist: That sounds tough. Have you tried calming routines before bed?\"}\n",
        "{\"text\": \"User: I'm doing okay today.\\nAI Therapist: That's great to hear. Celebrate the small wins.\"}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42a5CMs5vSJb",
        "outputId": "14d76891-f4a1-40b2-8c79-bd01b6d492c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': \"User: I'm doing okay today.\\nAI Therapist: That's great to hear. Celebrate the small wins.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_screen_time(df):\n",
        "    readable = df.to_string(index=False)\n",
        "\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a digital wellness AI.\n",
        "\n",
        "Analyze this screen time data:\n",
        "- Focus vs distraction\n",
        "- Burnout, overuse, addiction\n",
        "- Decision fatigue signs\n",
        "\n",
        "Recommend:\n",
        "- Mental clarity (0-100)\n",
        "- Avoid apps\n",
        "- 3 inspiring movies and calming songs\n",
        "- 3 digital detox habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"clarity_score\": 0, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Screen Time Data:\n",
        "{data}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"data\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(data=readable)\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "\n",
        "    try:\n",
        "        data = json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"‚ö†Ô∏è JSON parsing failed in screen time analysis. Raw output:\\n\", text)\n",
        "        return {\"clarity_score\": 50, \"fatigue\": \"Unknown\", \"avoid_apps\": [], \"recommend\": {\"movies\": [], \"songs\": []}, \"habits\": []}\n",
        "\n",
        "    # ‚úÖ Validation & fallback handling\n",
        "    if not isinstance(data, dict):\n",
        "        print(\"‚ö†Ô∏è Screen time JSON result is not a dictionary.\")\n",
        "        return {\"clarity_score\": 50, \"fatigue\": \"Unknown\", \"avoid_apps\": [], \"recommend\": {\"movies\": [], \"songs\": []}, \"habits\": []}\n",
        "\n",
        "    # Clamp clarity score between 0‚Äì100\n",
        "    try:\n",
        "        val = int(data.get(\"clarity_score\", 50))\n",
        "        data[\"clarity_score\"] = max(0, min(100, val))\n",
        "    except Exception:\n",
        "        print(\"‚ö†Ô∏è Invalid clarity_score, setting default 50\")\n",
        "        data[\"clarity_score\"] = 50\n",
        "\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "LlXj70AsxD-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import HumanMessage\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import time\n",
        "\n",
        "# Load environment variables and OpenAI key\n",
        "load_dotenv()\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "api_key = openai.api_key or \"sk-your-fallback-key\"\n",
        "print(\"‚úÖ Loaded OpenAI Key:\", bool(api_key))\n",
        "\n",
        "# Initialize LangChain LLM with explicit API key\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.5, openai_api_key=api_key)\n",
        "\n",
        "# --- WhatsApp Chat Extraction ---\n",
        "def extract_whatsapp_messages(zip_path):\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            file_list = zip_ref.namelist()\n",
        "            print(\"üîç Files in ZIP:\", file_list)\n",
        "            txt_files = [f for f in file_list if f.endswith('.txt')]\n",
        "            if not txt_files:\n",
        "                print(\"/content/whatsapp_chat_analysis.zip.\")\n",
        "                return []\n",
        "            chat_file = txt_files[0]\n",
        "            with zip_ref.open(chat_file) as f:\n",
        "                try:\n",
        "                    chat_data = f.read().decode('utf-8')\n",
        "                except UnicodeDecodeError:\n",
        "                    chat_data = f.read().decode('latin1')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå ZIP file not found: {zip_path}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Failed to extract chat:\", e)\n",
        "        return []\n",
        "\n",
        "    # Join multiline messages: lines NOT starting with date pattern belong to previous line\n",
        "    lines = chat_data.split('\\n')\n",
        "    merged_lines = []\n",
        "    date_pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ')\n",
        "    buffer = \"\"\n",
        "    for line in lines:\n",
        "        if date_pattern.match(line):\n",
        "            if buffer:\n",
        "                merged_lines.append(buffer)\n",
        "            buffer = line\n",
        "        else:\n",
        "            buffer += \" \" + line.strip()\n",
        "    if buffer:\n",
        "        merged_lines.append(buffer)\n",
        "\n",
        "    pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ([^:]+): (.+)$')\n",
        "    messages = []\n",
        "    for line in merged_lines:\n",
        "        match = pattern.match(line)\n",
        "        if match:\n",
        "            sender, msg = match.groups()\n",
        "            if msg.strip() and \"media omitted\" not in msg.lower():\n",
        "                messages.append(f\"{sender}: {msg}\")\n",
        "    print(f\"‚úÖ Extracted {len(messages)} messages from chat\")\n",
        "    return messages\n",
        "\n",
        "# --- WhatsApp Chat Analysis ---\n",
        "def analyze_chat(messages, n=50):\n",
        "    recent = \"\\n\".join(messages[-n:])\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a futuristic AI therapist from 2030.\n",
        "\n",
        "Analyze these WhatsApp messages:\n",
        "- Emotional tone (stress, joy, anxiety)\n",
        "- Mental clarity & decision style\n",
        "- Mindset type: proactive, reactive, balanced\n",
        "\n",
        "Recommend:\n",
        "- 3 apps/habits to avoid\n",
        "- 3 uplifting movies and songs\n",
        "- 3 good daily mental health habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"emotional_tone\": \"...\", \"clarity\": \"...\", \"mindset\": \"...\", \"avoid\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Chat:\n",
        "{chat}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"chat\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(chat=recent)\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"‚ö†Ô∏è JSON parsing failed in chat analysis. Raw output:\\n\", text)\n",
        "        return text\n",
        "\n",
        "# --- Screen Time Analysis ---\n",
        "def load_screen_time(csv_path):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"‚úÖ Loaded screen time data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load screen time CSV: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def analyze_screen_time(df):\n",
        "    readable = df.to_string(index=False)\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a digital wellness AI.\n",
        "\n",
        "Analyze this screen time data:\n",
        "- Focus vs distraction\n",
        "- Burnout, overuse, addiction\n",
        "- Decision fatigue signs\n",
        "\n",
        "Recommend:\n",
        "- Mental clarity (0-100)\n",
        "- Avoid apps\n",
        "- 3 inspiring movies and calming songs\n",
        "- 3 digital detox habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"clarity_score\": 0, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Screen Time Data:\n",
        "{data}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"data\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(data=readable)\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "\n",
        "    def validate_screen_time_json(data):\n",
        "      if not isinstance(data, dict):\n",
        "        print(\"‚ö†Ô∏è Screen time data not a dict, returning None\")\n",
        "        return None\n",
        "    df[\"clarity_score\"]\n",
        "    results.get(\"clarity_score\", 50)\n",
        "\n",
        "    if \"clarity_score\" in df.columns:\n",
        "        clarity = df[\"clarity_score\"]\n",
        "    else:\n",
        "        print(\"Column 'clarity_score' not found.\")\n",
        "        clarity = None  # or some default value\n",
        "        try:\n",
        "            val = int(data[\"clarity_score\"])\n",
        "            data[\"clarity_score\"] = max(0, min(100, val))\n",
        "        except Exception:\n",
        "            print(\"‚ö†Ô∏è 'clarity_score' invalid type, setting default 50\")\n",
        "            data[\"clarity_score\"] = 50\n",
        "\n",
        "    # Optional: Check other keys if necessary and fill defaults or clean\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# --- Twitter Sentiment Analysis ---\n",
        "def analyze_tweets(df):\n",
        "    print(\"üîç Columns in CSV:\", df.columns.tolist())\n",
        "    tweet_col = None\n",
        "    for col in df.columns:\n",
        "        if col.strip().lower() in [\"tweet\", \"text\", \"message\", \"content\"]:\n",
        "            tweet_col = col\n",
        "            break\n",
        "    if not tweet_col:\n",
        "        string_cols = df.select_dtypes(include='object')\n",
        "        tweet_col = string_cols.apply(lambda c: c.str.len().mean()).idxmax()\n",
        "        print(f\"‚úÖ Auto-selected tweet column: '{tweet_col}'\")\n",
        "\n",
        "    def analyze_sentiment_llm(tweet):\n",
        "        prompt = f\"\"\"\n",
        "You are a sentiment expert. Classify the tweet as one word: Positive, Negative, or Neutral.\n",
        "\n",
        "Tweet: \"{tweet}\"\n",
        "Sentiment:\"\"\"\n",
        "        try:\n",
        "            res = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0\n",
        "            )\n",
        "            sentiment = res['choices'][0]['message']['content'].strip()\n",
        "            # Normalize output\n",
        "            if sentiment.lower() not in [\"positive\", \"negative\", \"neutral\"]:\n",
        "                return \"Neutral\"\n",
        "            return sentiment\n",
        "        except Exception as e:\n",
        "            print(\"‚ö†Ô∏è Error analyzing tweet:\", e)\n",
        "            return \"Error\"\n",
        "\n",
        "    tqdm.pandas()\n",
        "    df[\"sentiment\"] = df[tweet_col].progress_apply(analyze_sentiment_llm)\n",
        "    return df\n",
        "\n",
        "# --- Final Report Synthesis ---\n",
        "def synthesize_report(chat_json, screen_json, sentiment_df):\n",
        "    sentiment_counts = sentiment_df[\"sentiment\"].value_counts().to_dict()\n",
        "    sentiment_summary = f\"Sentiment counts: {sentiment_counts}\"\n",
        "\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a NeuroAI fusion advisor from the future.\n",
        "\n",
        "Combine these:\n",
        "1. WhatsApp analysis:\n",
        "{chat_json}\n",
        "\n",
        "2. Screen time report:\n",
        "{screen_json}\n",
        "\n",
        "3. Twitter sentiment:\n",
        "{sentiments}\n",
        "\n",
        "Summarize teen mental health:\n",
        "- Mood and stress pattern\n",
        "- Top 3 issues\n",
        "- Mindfulness movie/song list\n",
        "- Futuristic habit suggestions\n",
        "\n",
        "Respond warmly and clearly.\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"chat_json\", \"screen_json\", \"sentiments\"],\n",
        "        template=prompt_template_str\n",
        "    )\n",
        "    formatted_prompt = prompt.format(\n",
        "        chat_json=json.dumps(chat_json, indent=2) if isinstance(chat_json, dict) else str(chat_json),\n",
        "        screen_json=json.dumps(screen_json, indent=2) if isinstance(screen_json, dict) else str(screen_json),\n",
        "        sentiments=sentiment_summary\n",
        "    )\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "    return text\n",
        "\n",
        "# --- MAIN PIPELINE ---\n",
        "if __name__ == \"__main__\":\n",
        "    whatsapp_file = \"whatsapp_chat_analysis.zip\"\n",
        "    screen_time_file = \"screentime_analysis.csv\"\n",
        "    twitter_file = \"teen_tweets.csv\"\n",
        "\n",
        "    try:\n",
        "        # 1. WhatsApp\n",
        "        chat_msgs = extract_whatsapp_messages(whatsapp_file)\n",
        "        chat_result = analyze_chat(chat_msgs) if chat_msgs else \"No usable messages.\"\n",
        "\n",
        "        # 2. Screen Time\n",
        "        df_screen = load_screen_time(screen_time_file)\n",
        "        screen_result = analyze_screen_time(df_screen) if not df_screen.empty else \"No screen time data.\"\n",
        "\n",
        "        # 3. Twitter Sentiment\n",
        "        df_tweets = pd.read_csv(twitter_file)\n",
        "        sentiment_df = analyze_tweets(df_tweets)\n",
        "\n",
        "        # 4. Final Mental Health Report\n",
        "        final_report = synthesize_report(chat_result, screen_result, sentiment_df)\n",
        "        print(\"\\nüß† Final Mental Health Summary:\\n\", final_report)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\" Pipeline :\", e)\n",
        "\n",
        "    import json\n",
        "\n",
        "# --- INPUT JSON ---\n",
        "data = '''\n",
        "{\n",
        "  \"mood\": \"Stressed and anxious with periods of joy\",\n",
        "  \"top_issues\": [\"Overuse of social media\", \"Sleep deprivation\", \"Lack of mental clarity\"],\n",
        "  \"recommended_movies\": [\"Inside Out\", \"The Pursuit of Happyness\", \"Soul\"],\n",
        "  \"recommended_songs\": [\"Weightless - Marconi Union\", \"Lovely Day - Bill Withers\", \"Here Comes the Sun - The Beatles\"],\n",
        "  \"habits\": [\"Daily journaling\", \"30-minute screen-free walk\", \"Night-time digital detox routine\"]\n",
        "}\n",
        "'''\n",
        "\n",
        "# --- LOAD JSON INTO PYTHON DICTIONARY ---\n",
        "analysis = json.loads(data)\n",
        "\n",
        "# --- PRINT IN A CLEAN FORMAT ---\n",
        "print(\"\\nüß† Mood Summary:\")\n",
        "print(f\"  - Mood: {analysis['mood']}\")\n",
        "\n",
        "print(\"\\nüö© Top Issues Detected:\")\n",
        "for issue in analysis['top_issues']:\n",
        "    print(f\"  - {issue}\")\n",
        "\n",
        "print(\"\\nüé¨ Recommended Movies:\")\n",
        "for movie in analysis['recommended_movies']:\n",
        "    print(f\"  - {movie}\")\n",
        "\n",
        "print(\"\\nüéµ Recommended Songs:\")\n",
        "for song in analysis['recommended_songs']:\n",
        "    print(f\"  - {song}\")\n",
        "\n",
        "print(\"\\nüåø Suggested Mental Health Habits:\")\n",
        "for habit in analysis['habits']:\n",
        "    print(f\"  - {habit}\")\n",
        "\n",
        "\n",
        "def analyze_screen_time(df):\n",
        "    readable = df.to_string(index=False)\n",
        "\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a digital wellness AI.\n",
        "\n",
        "Analyze this screen time data:\n",
        "- Focus vs distraction\n",
        "- Burnout, overuse, addiction\n",
        "- Decision fatigue signs\n",
        "\n",
        "Recommend:\n",
        "- Mental clarity (0-100)\n",
        "- Avoid apps\n",
        "- 3 inspiring movies and calming songs\n",
        "- 3 digital detox habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"clarity_score\": 0, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Screen Time Data:\n",
        "{data}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"data\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(data=readable)\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "\n",
        "    try:\n",
        "        data = json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"‚ö†Ô∏è JSON parsing failed in screen time analysis. Raw output:\\n\", text)\n",
        "        return {\"clarity_score\": 50, \"fatigue\": \"Unknown\", \"avoid_apps\": [], \"recommend\": {\"movies\": [], \"songs\": []}, \"habits\": []}\n",
        "\n",
        "    # ‚úÖ Validation & fallback handling\n",
        "    if not isinstance(data, dict):\n",
        "        print(\"‚ö†Ô∏è Screen time JSON result is not a dictionary.\")\n",
        "        return {\"clarity_score\": 50, \"fatigue\": \"Unknown\", \"avoid_apps\": [], \"recommend\": {\"movies\": [], \"songs\": []}, \"habits\": []}\n",
        "\n",
        "    # Clamp clarity score between 0‚Äì100\n",
        "    try:\n",
        "        val = int(data.get(\"clarity_score\", 50))\n",
        "        data[\"clarity_score\"] = max(0, min(100, val))\n",
        "    except Exception:\n",
        "        print(\"‚ö†Ô∏è Invalid clarity_score, setting default 50\")\n",
        "        data[\"clarity_score\"] = 50\n",
        "\n",
        "    return data\n",
        "print(f\"üß† Mood: {analysis['mood']} | üö© Issues: {', '.join(analysis['top_issues'])} | üé¨ Movies: {', '.join(analysis['recommended_movies'])} | üéµ Songs: {', '.join(analysis['recommended_songs'])} | üåø Habits: {', '.join(analysis['habits'])}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmLnYSLrxSH_",
        "outputId": "0483737e-db25-467b-a581-70a29396c651"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded OpenAI Key: True\n",
            "üîç Files in ZIP: ['whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/1.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/10.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/11.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/12.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/13.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/14.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/15.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/16.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/17.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/18.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/19.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/2.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/20.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/21.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/3.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/4.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/5.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/6.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/7.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/8.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/9.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/A+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/Dataset.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/L+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/both+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/both.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_NLTK_neg.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_NLTK_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_spanish_neg.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_spanish_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_text_blob_down.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_text_blob_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/translated.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/README.md', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Whatsapp_Analysis.ipynb']\n",
            "/content/whatsapp_chat_analysis.zip.\n",
            "‚úÖ Loaded screen time data: 200 rows, 5 columns\n",
            " Pipeline : '\"clarity_score\"'\n",
            "\n",
            "üß† Mood Summary:\n",
            "  - Mood: Stressed and anxious with periods of joy\n",
            "\n",
            "üö© Top Issues Detected:\n",
            "  - Overuse of social media\n",
            "  - Sleep deprivation\n",
            "  - Lack of mental clarity\n",
            "\n",
            "üé¨ Recommended Movies:\n",
            "  - Inside Out\n",
            "  - The Pursuit of Happyness\n",
            "  - Soul\n",
            "\n",
            "üéµ Recommended Songs:\n",
            "  - Weightless - Marconi Union\n",
            "  - Lovely Day - Bill Withers\n",
            "  - Here Comes the Sun - The Beatles\n",
            "\n",
            "üåø Suggested Mental Health Habits:\n",
            "  - Daily journaling\n",
            "  - 30-minute screen-free walk\n",
            "  - Night-time digital detox routine\n",
            "üß† Mood: Stressed and anxious with periods of joy | üö© Issues: Overuse of social media, Sleep deprivation, Lack of mental clarity | üé¨ Movies: Inside Out, The Pursuit of Happyness, Soul | üéµ Songs: Weightless - Marconi Union, Lovely Day - Bill Withers, Here Comes the Sun - The Beatles | üåø Habits: Daily journaling, 30-minute screen-free walk, Night-time digital detox routine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import HumanMessage\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import time\n",
        "\n",
        "# Load environment variables and OpenAI key\n",
        "load_dotenv()\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "api_key = openai.api_key or \"sk-your-fallback-key\"\n",
        "print(\"‚úÖ Loaded OpenAI Key:\", bool(api_key))\n",
        "\n",
        "# Initialize LangChain LLM with explicit API key\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.5, openai_api_key=api_key)\n",
        "\n",
        "# --- WhatsApp Chat Extraction ---\n",
        "def extract_whatsapp_messages(zip_path):\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            file_list = zip_ref.namelist()\n",
        "            print(\"üîç Files in ZIP:\", file_list)\n",
        "            txt_files = [f for f in file_list if f.endswith('.txt')]\n",
        "            if not txt_files:\n",
        "                print(\"/content/whatsapp_chat_analysis.zip.\")\n",
        "                return []\n",
        "            chat_file = txt_files[0]\n",
        "            with zip_ref.open(chat_file) as f:\n",
        "                try:\n",
        "                    chat_data = f.read().decode('utf-8')\n",
        "                except UnicodeDecodeError:\n",
        "                    chat_data = f.read().decode('latin1')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå ZIP file not found: {zip_path}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Failed to extract chat:\", e)\n",
        "        return []\n",
        "\n",
        "    # Join multiline messages: lines NOT starting with date pattern belong to previous line\n",
        "    lines = chat_data.split('\\n')\n",
        "    merged_lines = []\n",
        "    date_pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ')\n",
        "    buffer = \"\"\n",
        "    for line in lines:\n",
        "        if date_pattern.match(line):\n",
        "            if buffer:\n",
        "                merged_lines.append(buffer)\n",
        "            buffer = line\n",
        "        else:\n",
        "            buffer += \" \" + line.strip()\n",
        "    if buffer:\n",
        "        merged_lines.append(buffer)\n",
        "\n",
        "    pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ([^:]+): (.+)$')\n",
        "    messages = []\n",
        "    for line in merged_lines:\n",
        "        match = pattern.match(line)\n",
        "        if match:\n",
        "            sender, msg = match.groups()\n",
        "            if msg.strip() and \"media omitted\" not in msg.lower():\n",
        "                messages.append(f\"{sender}: {msg}\")\n",
        "    print(f\"‚úÖ Extracted {len(messages)} messages from chat\")\n",
        "    return messages\n",
        "\n",
        "# --- WhatsApp Chat Analysis ---\n",
        "def analyze_chat(messages, n=50):\n",
        "    recent = \"\\n\".join(messages[-n:])\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a futuristic AI therapist from 2030.\n",
        "\n",
        "Analyze these WhatsApp messages:\n",
        "- Emotional tone (stress, joy, anxiety)\n",
        "- Mental clarity & decision style\n",
        "- Mindset type: proactive, reactive, balanced\n",
        "\n",
        "Recommend:\n",
        "- 3 apps/habits to avoid\n",
        "- 3 uplifting movies and songs\n",
        "- 3 good daily mental health habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"emotional_tone\": \"...\", \"clarity\": \"...\", \"mindset\": \"...\", \"avoid\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Chat:\n",
        "{chat}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"chat\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(chat=recent)\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"‚ö†Ô∏è JSON parsing failed in chat analysis. Raw output:\\n\", text)\n",
        "        return text\n",
        "\n",
        "# --- Screen Time Analysis ---\n",
        "def load_screen_time(csv_path):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"‚úÖ Loaded screen time data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load screen time CSV: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def analyze_screen_time(df):\n",
        "    readable = df.to_string(index=False)\n",
        "\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a digital wellness AI.\n",
        "\n",
        "Analyze this screen time data:\n",
        "- Focus vs distraction\n",
        "- Burnout, overuse, addiction\n",
        "- Decision fatigue signs\n",
        "\n",
        "Recommend:\n",
        "- Mental clarity (0-100)\n",
        "- Avoid apps\n",
        "- 3 inspiring movies and calming songs\n",
        "- 3 digital detox habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"clarity_score\": 0, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Screen Time Data:\n",
        "{data}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"data\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(data=readable)\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "\n",
        "    try:\n",
        "        data = json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"‚ö†Ô∏è JSON parsing failed in screen time analysis. Raw output:\\n\", text)\n",
        "        return {\"clarity_score\": 50, \"fatigue\": \"Unknown\", \"avoid_apps\": [], \"recommend\": {\"movies\": [], \"songs\": []}, \"habits\": []}\n",
        "\n",
        "    # ‚úÖ Validation & fallback handling\n",
        "    if not isinstance(data, dict):\n",
        "        print(\"‚ö†Ô∏è Screen time JSON result is not a dictionary.\")\n",
        "        return {\"clarity_score\": 50, \"fatigue\": \"Unknown\", \"avoid_apps\": [], \"recommend\": {\"movies\": [], \"songs\": []}, \"habits\": []}\n",
        "\n",
        "    try:\n",
        "        val = int(data.get(\"clarity_score\", 50))\n",
        "        data[\"clarity_score\"] = max(0, min(100, val))\n",
        "    except Exception:\n",
        "        print(\"‚ö†Ô∏è Invalid clarity_score, setting default 50\")\n",
        "        data[\"clarity_score\"] = 50\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "# --- Twitter Sentiment Analysis ---\n",
        "def analyze_tweets(df):\n",
        "    print(\"üîç Columns in CSV:\", df.columns.tolist())\n",
        "    tweet_col = None\n",
        "    for col in df.columns:\n",
        "        if col.strip().lower() in [\"tweet\", \"text\", \"message\", \"content\"]:\n",
        "            tweet_col = col\n",
        "            break\n",
        "    if not tweet_col:\n",
        "        string_cols = df.select_dtypes(include='object')\n",
        "        tweet_col = string_cols.apply(lambda c: c.str.len().mean()).idxmax()\n",
        "        print(f\"‚úÖ Auto-selected tweet column: '{tweet_col}'\")\n",
        "\n",
        "    def analyze_sentiment_llm(tweet):\n",
        "        prompt = f\"\"\"\n",
        "You are a sentiment expert. Classify the tweet as one word: Positive, Negative, or Neutral.\n",
        "\n",
        "Tweet: \"{tweet}\"\n",
        "Sentiment:\"\"\"\n",
        "        try:\n",
        "            res = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0\n",
        "            )\n",
        "            sentiment = res['choices'][0]['message']['content'].strip()\n",
        "            # Normalize output\n",
        "            if sentiment.lower() not in [\"positive\", \"negative\", \"neutral\"]:\n",
        "                return \"Neutral\"\n",
        "            return sentiment\n",
        "        except Exception as e:\n",
        "            print(\"‚ö†Ô∏è Error analyzing tweet:\", e)\n",
        "            return \"Error\"\n",
        "\n",
        "    tqdm.pandas()\n",
        "    df[\"sentiment\"] = df[tweet_col].progress_apply(analyze_sentiment_llm)\n",
        "    return df\n",
        "\n",
        "# --- Final Report Synthesis ---\n",
        "def synthesize_report(chat_json, screen_json, sentiment_df):\n",
        "    sentiment_counts = sentiment_df[\"sentiment\"].value_counts().to_dict()\n",
        "    sentiment_summary = f\"Sentiment counts: {sentiment_counts}\"\n",
        "\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a NeuroAI fusion advisor from the future.\n",
        "\n",
        "Combine these:\n",
        "1. WhatsApp analysis:\n",
        "{chat_json}\n",
        "\n",
        "2. Screen time report:\n",
        "{screen_json}\n",
        "\n",
        "3. Twitter sentiment:\n",
        "{sentiments}\n",
        "\n",
        "Summarize teen mental health:\n",
        "- Mood and stress pattern\n",
        "- Top 3 issues\n",
        "- Mindfulness movie/song list\n",
        "- Futuristic habit suggestions\n",
        "\n",
        "Respond warmly and clearly.\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"chat_json\", \"screen_json\", \"sentiments\"],\n",
        "        template=prompt_template_str\n",
        "    )\n",
        "    formatted_prompt = prompt.format(\n",
        "        chat_json=json.dumps(chat_json, indent=2) if isinstance(chat_json, dict) else str(chat_json),\n",
        "        screen_json=json.dumps(screen_json, indent=2) if isinstance(screen_json, dict) else str(screen_json),\n",
        "        sentiments=sentiment_summary\n",
        "    )\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "    return text\n",
        "\n",
        "# --- MAIN PIPELINE ---\n",
        "if __name__ == \"__main__\":\n",
        "    whatsapp_file = \"whatsapp_chat_analysis.zip\"\n",
        "    screen_time_file = \"screentime_analysis.csv\"\n",
        "    twitter_file = \"teen_tweets.csv\"\n",
        "\n",
        "    try:\n",
        "        # 1. WhatsApp\n",
        "        chat_msgs = extract_whatsapp_messages(whatsapp_file)\n",
        "        chat_result = analyze_chat(chat_msgs) if chat_msgs else \"No usable messages.\"\n",
        "\n",
        "        # 2. Screen Time\n",
        "        df_screen = load_screen_time(screen_time_file)\n",
        "        screen_result = analyze_screen_time(df_screen) if not df_screen.empty else \"No screen time data.\"\n",
        "\n",
        "        # 3. Twitter Sentiment\n",
        "        df_tweets = pd.read_csv(twitter_file)\n",
        "        sentiment_df = analyze_tweets(df_tweets)\n",
        "\n",
        "        # 4. Final Mental Health Report\n",
        "        final_report = synthesize_report(chat_result, screen_result, sentiment_df)\n",
        "        print(\"\\nüß† Final Mental Health Summary:\\n\", final_report)\n",
        "        print(final_report)  # safer and already used in the main pipeline\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\" Pipeline :\", e)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "def analyze_screen_time(df):\n",
        "    readable = df.to_string(index=False)\n",
        "\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a digital wellness AI.\n",
        "\n",
        "Analyze this screen time data:\n",
        "- Focus vs distraction\n",
        "- Burnout, overuse, addiction\n",
        "- Decision fatigue signs\n",
        "\n",
        "Recommend:\n",
        "- Mental clarity (0-100)\n",
        "- Avoid apps\n",
        "- 3 inspiring movies and calming songs\n",
        "- 3 digital detox habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"clarity_score\": 0, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Screen Time Data:\n",
        "{data}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"data\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(data=readable)\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "\n",
        "    try:\n",
        "        data = json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"‚ö†Ô∏è JSON parsing failed in screen time analysis. Raw output:\\n\", text)\n",
        "        return {\"clarity_score\": 50, \"fatigue\": \"Unknown\", \"avoid_apps\": [], \"recommend\": {\"movies\": [], \"songs\": []}, \"habits\": []}\n",
        "\n",
        "    # ‚úÖ Validation & fallback handling\n",
        "    if not isinstance(data, dict):\n",
        "        print(\"‚ö†Ô∏è Screen time JSON result is not a dictionary.\")\n",
        "        return {\"clarity_score\": 50, \"fatigue\": \"Unknown\", \"avoid_apps\": [], \"recommend\": {\"movies\": [], \"songs\": []}, \"habits\": []}\n",
        "\n",
        "    # Clamp clarity score between 0‚Äì100\n",
        "    try:\n",
        "        val = int(data.get(\"clarity_score\", 50))\n",
        "        data[\"clarity_score\"] = max(0, min(100, val))\n",
        "    except Exception:\n",
        "        print(\"‚ö†Ô∏è Invalid clarity_score, setting default 50\")\n",
        "        data[\"clarity_score\"] = 50\n",
        "\n",
        "    return data\n",
        "\n",
        "print(f\"üß† Mood: {analysis['mood']} | üö© Issues: {', '.join(analysis['top_issues'])} | üé¨ Movies: {', '.join(analysis['recommended_movies'])} | üéµ Songs: {', '.join(analysis['recommended_songs'])} | üåø Habits: {', '.join(analysis['habits'])}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHtXb2lrm_3x",
        "outputId": "029f9639-3039-4ee7-8d0d-be411583be0a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded OpenAI Key: True\n",
            "üîç Files in ZIP: ['whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/1.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/10.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/11.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/12.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/13.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/14.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/15.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/16.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/17.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/18.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/19.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/2.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/20.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/21.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/3.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/4.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/5.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/6.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/7.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/8.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/9.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/A+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/Dataset.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/L+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/both+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/both.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_NLTK_neg.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_NLTK_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_spanish_neg.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_spanish_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_text_blob_down.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_text_blob_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/translated.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/README.md', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Whatsapp_Analysis.ipynb']\n",
            "/content/whatsapp_chat_analysis.zip.\n",
            "‚úÖ Loaded screen time data: 200 rows, 5 columns\n",
            " Pipeline : '\"clarity_score\"'\n",
            "üß† Mood: Stressed and anxious with periods of joy | üö© Issues: Overuse of social media, Sleep deprivation, Lack of mental clarity | üé¨ Movies: Inside Out, The Pursuit of Happyness, Soul | üéµ Songs: Weightless - Marconi Union, Lovely Day - Bill Withers, Here Comes the Sun - The Beatles | üåø Habits: Daily journaling, 30-minute screen-free walk, Night-time digital detox routine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qLDikbqufoY",
        "outputId": "3738c16b-57d5-4b63-f804-f7223c67d02e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/590.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m583.7/590.6 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import HumanMessage\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "\n",
        "# Load environment variables and API key\n",
        "load_dotenv()\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "api_key = openai.api_key or \"sk-your-fallback-key\"\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.5, openai_api_key=api_key)\n",
        "\n",
        "# --- WhatsApp Chat Extraction ---\n",
        "def extract_whatsapp_messages(zip_path):\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            txt_files = [f for f in zip_ref.namelist() if f.endswith('.txt')]\n",
        "            if not txt_files:\n",
        "                return []\n",
        "            with zip_ref.open(txt_files[0]) as f:\n",
        "                try:\n",
        "                    chat_data = f.read().decode('utf-8')\n",
        "                except UnicodeDecodeError:\n",
        "                    chat_data = f.read().decode('latin1')\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Error extracting chat:\", e)\n",
        "        return []\n",
        "\n",
        "    lines = chat_data.split('\\n')\n",
        "    merged_lines = []\n",
        "    date_pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ')\n",
        "    buffer = \"\"\n",
        "    for line in lines:\n",
        "        if date_pattern.match(line):\n",
        "            if buffer:\n",
        "                merged_lines.append(buffer)\n",
        "            buffer = line\n",
        "        else:\n",
        "            buffer += \" \" + line.strip()\n",
        "    if buffer:\n",
        "        merged_lines.append(buffer)\n",
        "\n",
        "    pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ([^:]+): (.+)$')\n",
        "    messages = [f\"{m.group(1)}: {m.group(2)}\" for m in map(pattern.match, merged_lines) if m and \"media omitted\" not in m.group(2).lower()]\n",
        "    return messages\n",
        "\n",
        "# --- WhatsApp Chat Analysis ---\n",
        "def analyze_chat(messages, n=50):\n",
        "    recent = \"\\n\".join(messages[-n:])\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=[\"chat\"],\n",
        "        template=\"\"\"\n",
        "You are a futuristic AI therapist from 2030.\n",
        "\n",
        "Analyze these WhatsApp messages:\n",
        "- Emotional tone (stress, joy, anxiety)\n",
        "- Mental clarity & decision style\n",
        "- Mindset type: proactive, reactive, balanced\n",
        "\n",
        "Recommend:\n",
        "- 3 apps/habits to avoid\n",
        "- 3 uplifting movies and songs\n",
        "- 3 good daily mental health habits\n",
        "\n",
        "Output ONLY in JSON format:\n",
        "{\"emotional_tone\": \"...\", \"clarity\": \"...\", \"mindset\": \"...\", \"avoid\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Chat:\n",
        "{chat}\n",
        "\"\"\"\n",
        "    )\n",
        "    response = llm([HumanMessage(content=prompt_template.format(chat=recent))])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è Chat JSON parse error:\\n\", text)\n",
        "        return text\n",
        "\n",
        "# --- Screen Time Analysis ---\n",
        "def load_screen_time(csv_path):\n",
        "    try:\n",
        "        return pd.read_csv(csv_path)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load screen time CSV: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def analyze_screen_time(df):\n",
        "    readable = df.to_string(index=False)\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=[\"data\"],\n",
        "        template=\"\"\"\n",
        "You are a digital wellness AI.\n",
        "\n",
        "Analyze this screen time data:\n",
        "- Focus vs distraction\n",
        "- Burnout, overuse, addiction\n",
        "- Decision fatigue signs\n",
        "\n",
        "Recommend:\n",
        "- Mental clarity (0-100)\n",
        "- Avoid apps\n",
        "- 3 inspiring movies and calming songs\n",
        "- 3 digital detox habits\n",
        "\n",
        "Output ONLY in JSON format:\n",
        "{\"clarity_score\": 0, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Screen Time Data:\n",
        "{data}\n",
        "\"\"\"\n",
        "    )\n",
        "    response = llm([HumanMessage(content=prompt_template.format(data=readable))])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "    try:\n",
        "        data = json.loads(text)\n",
        "        data[\"clarity_score\"] = max(0, min(100, int(data.get(\"clarity_score\", 50))))\n",
        "        return data\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è Screen time JSON parse error:\\n\", text)\n",
        "        return {\"clarity_score\": 50, \"fatigue\": \"Unknown\", \"avoid_apps\": [], \"recommend\": {\"movies\": [], \"songs\": []}, \"habits\": []}\n",
        "\n",
        "# --- Twitter Sentiment Analysis ---\n",
        "def analyze_tweets(df):\n",
        "    tweet_col = next((col for col in df.columns if col.lower() in [\"tweet\", \"text\", \"message\", \"content\"]), None)\n",
        "    if not tweet_col:\n",
        "        tweet_col = df.select_dtypes(include='object').apply(lambda c: c.str.len().mean()).idxmax()\n",
        "\n",
        "    def analyze_sentiment(tweet):\n",
        "        prompt = f'Tweet: \"{tweet}\"\\nClassify as one word: Positive, Negative, or Neutral.'\n",
        "        try:\n",
        "            res = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0\n",
        "            )\n",
        "            sentiment = res['choices'][0]['message']['content'].strip().capitalize()\n",
        "            return sentiment if sentiment in [\"Positive\", \"Negative\", \"Neutral\"] else \"Neutral\"\n",
        "        except:\n",
        "            return \"Error\"\n",
        "\n",
        "    tqdm.pandas()\n",
        "    df[\"sentiment\"] = df[tweet_col].progress_apply(analyze_sentiment)\n",
        "    return df\n",
        "\n",
        "# --- Final Report ---\n",
        "def synthesize_report(chat_json, screen_json, sentiment_df):\n",
        "    sentiment_summary = sentiment_df[\"sentiment\"].value_counts().to_dict()\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"chat_json\", \"screen_json\", \"sentiments\"],\n",
        "        template=\"\"\"\n",
        "You are a NeuroAI advisor.\n",
        "\n",
        "Combine:\n",
        "1. WhatsApp analysis: {chat_json}\n",
        "2. Screen time report: {screen_json}\n",
        "3. Twitter sentiment: {sentiments}\n",
        "\n",
        "Summarize teen mental health:\n",
        "- Mood and stress\n",
        "- Top 3 issues\n",
        "- Mindful movie/song list\n",
        "- 3 futuristic daily mental health habits\n",
        "\n",
        "Respond in natural tone.\n",
        "\"\"\"\n",
        "    )\n",
        "    full_prompt = prompt.format(\n",
        "        chat_json=json.dumps(chat_json, indent=2),\n",
        "        screen_json=json.dumps(screen_json, indent=2),\n",
        "        sentiments=sentiment_summary\n",
        "    )\n",
        "    response = llm([HumanMessage(content=full_prompt)])\n",
        "    return response[0].content if isinstance(response, list) else str(response)\n",
        "\n",
        "# --- MAIN PIPELINE ---\n",
        "if __name__ == \"__main__\":\n",
        "    whatsapp_file = \"whatsapp_chat_analysis.zip\"\n",
        "    screen_time_file = \"screentime_analysis.csv\"\n",
        "    twitter_file = \"teen_tweets.csv\"\n",
        "\n",
        "    try:\n",
        "        chat_msgs = extract_whatsapp_messages(whatsapp_file)\n",
        "        chat_result = analyze_chat(chat_msgs) if chat_msgs else \"No messages\"\n",
        "\n",
        "        screen_df = load_screen_time(screen_time_file)\n",
        "        screen_result = analyze_screen_time(screen_df) if not screen_df.empty else \"No screen data\"\n",
        "\n",
        "        tweets_df = pd.read_csv(twitter_file)\n",
        "        sentiment_df = analyze_tweets(tweets_df)\n",
        "\n",
        "        final_report = synthesize_report(chat_result, screen_result, sentiment_df)\n",
        "        print(\"\\nüß† Final Teen Mental Health Summary:\\n\")\n",
        "        print(final_report)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Pipeline Error:\", e)\n",
        "        print(f\"üß† Mood: {analysis['mood']} | üö© Issues: {', '.join(analysis['top_issues'])} | üé¨ Movies: {', '.join(analysis['recommended_movies'])} | üéµ Songs: {', '.join(analysis['recommended_songs'])} | üåø Habits: {', '.join(analysis['habits'])}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQD412dhJ_dU",
        "outputId": "bfef7364-c804-43bf-98f6-c9142fbbc12a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Pipeline Error: '\"clarity_score\"'\n",
            "üß† Mood: Stressed and anxious with periods of joy | üö© Issues: Overuse of social media, Sleep deprivation, Lack of mental clarity | üé¨ Movies: Inside Out, The Pursuit of Happyness, Soul | üéµ Songs: Weightless - Marconi Union, Lovely Day - Bill Withers, Here Comes the Sun - The Beatles | üåø Habits: Daily journaling, 30-minute screen-free walk, Night-time digital detox routine\n"
          ]
        }
      ]
    }
  ]
}