{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym2AXP2FgMKn",
        "outputId": "81b87293-67fe-41ed-ea4d-feaf2f2861eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pz6AU-WkgQhu",
        "outputId": "1f441ddd-b199-40a3-c52b-15201c046ff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.84.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.63)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.44)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VZG3wZmhH7H",
        "outputId": "ef707427-1e1c-488f-ee86-18938c0771b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.84.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.63)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.44)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.24 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai langchain langchain-openai python-dotenv pandas tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9bqhZLNiIsJ",
        "outputId": "dd60ce0d-3957-41a3-b095-9f665a00f058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.84.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.21-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.63)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.44)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting langchain-core<1.0.0,>=0.3.58 (from langchain)\n",
            "  Downloading langchain_core-0.3.64-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.3.45-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Downloading langchain_openai-0.3.21-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.64-py3-none-any.whl (438 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.1/438.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.3.45-py3-none-any.whl (363 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langsmith, langchain-core, langchain-openai\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.3.44\n",
            "    Uninstalling langsmith-0.3.44:\n",
            "      Successfully uninstalled langsmith-0.3.44\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.63\n",
            "    Uninstalling langchain-core-0.3.63:\n",
            "      Successfully uninstalled langchain-core-0.3.63\n",
            "Successfully installed langchain-core-0.3.64 langchain-openai-0.3.21 langsmith-0.3.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"sk-xxx\"  # Replace with your key\n"
      ],
      "metadata": {
        "id": "l2sC8tvUjGOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_screen_time_json(data):\n",
        "    if not isinstance(data, dict):\n",
        "        return None\n",
        "    if \"clarity_score\" not in data:\n",
        "        print(\"⚠️ clarity_score missing, setting default 50\")\n",
        "        data[\"clarity_score\"] = 50  # default mid-score\n",
        "    else:\n",
        "        # Try to convert to int safely\n",
        "        try:\n",
        "            val = int(data[\"clarity_score\"])\n",
        "            data[\"clarity_score\"] = max(0, min(100, val))  # clamp between 0-100\n",
        "        except:\n",
        "            print(\"⚠️ clarity_score invalid, setting default 50\")\n",
        "            data[\"clarity_score\"] = 50\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "Eva_Z427q4fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "\n",
        "# --- Load Keys ---\n",
        "load_dotenv()\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "api_key = openai.api_key or \"sk-your-fallback-key\"  # Replace if needed\n",
        "print(\"✅ Loaded OpenAI Key:\", bool(api_key))\n",
        "\n",
        "# ✅ LangChain LLM (explicit API key)\n",
        "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.5, openai_api_key=api_key)\n",
        "\n",
        "# --- WhatsApp Chat Extraction ---\n",
        "def extract_whatsapp_messages(zip_path):\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, '/content/whatsapp_chat_analysis.zip') as zip_ref:\n",
        "            file_list = zip_ref.namelist()\n",
        "            print(\"🔍 Files in ZIP:\", file_list)\n",
        "            txt_files = [f for f in file_list if f.endswith('.txt')]\n",
        "            if not txt_files:\n",
        "                print(\"❌ No .txt file found in ZIP archive.\")\n",
        "                return []\n",
        "            chat_file = txt_files[0]\n",
        "            with zip_ref.open(chat_file) as f:\n",
        "                try:\n",
        "                    chat_data = f.read().decode('utf-8')\n",
        "                except UnicodeDecodeError:\n",
        "                    chat_data = f.read().decode('latin1')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ ZIP file not found: {zip_path}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(\"❌ Failed to extract chat:\", e)\n",
        "        return []\n",
        "\n",
        "    pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ([^:]+): (.+)$')\n",
        "    messages = []\n",
        "    for line in chat_data.split(\"\\n\"):\n",
        "        match = pattern.match(line)\n",
        "        if match:\n",
        "            sender, msg = match.groups()\n",
        "            if msg.strip() and \"media omitted\" not in msg.lower():\n",
        "                messages.append(f\"{sender}: {msg}\")\n",
        "    return messages\n",
        "\n",
        "\n",
        "# --- WhatsApp Chat Analysis ---\n",
        "def analyze_chat(messages, n=50):\n",
        "    recent = \"\\n\".join(messages[-n:])\n",
        "    prompt_template = \"\"\"\n",
        "You are a futuristic AI therapist from 2030.\n",
        "\n",
        "Analyze these WhatsApp messages:\n",
        "- Emotional tone (stress, joy, anxiety)\n",
        "- Mental clarity & decision style\n",
        "- Mindset type: proactive, reactive, balanced\n",
        "\n",
        "Recommend:\n",
        "- 3 apps/habits to avoid\n",
        "- 3 uplifting movies and songs\n",
        "- 3 good daily mental health habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"emotional_tone\": \"...\", \"clarity\": \"...\", \"mindset\": \"...\", \"avoid\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Chat:\n",
        "{chat}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"chat\"], template=prompt_template)\n",
        "    formatted_prompt = prompt.format(chat=recent)\n",
        "    response = llm.invoke(formatted_prompt)\n",
        "\n",
        "    try:\n",
        "        return json.loads(response.content)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"⚠️ JSON parsing failed in chat analysis. Raw output:\\n\", response.content)\n",
        "        return response.content\n",
        "\n",
        "# --- Screen Time Analysis ---\n",
        "def load_screen_time(csv_path):\n",
        "    return pd.read_csv(csv_path)\n",
        "\n",
        "def analyze_screen_time(df):\n",
        "    readable = df.to_string(index=False)\n",
        "    prompt_template = \"\"\"\n",
        "You are a digital wellness AI.\n",
        "\n",
        "Analyze this screen time data:\n",
        "- Focus vs distraction\n",
        "- Burnout, overuse, addiction\n",
        "- Decision fatigue signs\n",
        "\n",
        "Recommend:\n",
        "- Mental clarity (0-100)\n",
        "- Avoid apps\n",
        "- 3 inspiring movies and calming songs\n",
        "- 3 digital detox habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"clarity_score\": 0-100, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Screen Time Data:\n",
        "{data}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"data\"], template=prompt_template)\n",
        "    formatted_prompt = prompt.format(data=readable)\n",
        "    response = llm.invoke(formatted_prompt)\n",
        "\n",
        "    try:\n",
        "        json_data = json.loads(response.content)\n",
        "        return validate_screen_time_json(json_data)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"⚠️ JSON parsing failed in screen time analysis. Raw output:\\n\", response.content)\n",
        "        return response.content\n",
        "\n",
        "# Validation function for clarity_score\n",
        "def validate_screen_time_json(data):\n",
        "    if not isinstance(data, dict):\n",
        "        print(\"⚠️ Screen time data not a dict, returning None\")\n",
        "        return None\n",
        "    if \"clarity_score\" not in data:\n",
        "        print(\"⚠️ 'clarity_score' missing, setting default 50\")\n",
        "        data[\"clarity_score\"] = 50\n",
        "    else:\n",
        "        try:\n",
        "            val = int(data[\"clarity_score\"])\n",
        "            data[\"clarity_score\"] = max(0, min(100, val))\n",
        "        except Exception:\n",
        "            print(\"⚠️ 'clarity_score' invalid, setting default 50\")\n",
        "            data[\"clarity_score\"] = 50\n",
        "    return data\n",
        "\n",
        "# --- Twitter Sentiment Analysis ---\n",
        "def analyze_tweets(df):\n",
        "    print(\"🔍 Columns in CSV:\", df.columns.tolist())\n",
        "    tweet_col = None\n",
        "    for col in df.columns:\n",
        "        if col.strip().lower() in [\"tweet\", \"text\", \"message\", \"content\"]:\n",
        "            tweet_col = col\n",
        "            break\n",
        "    if not tweet_col:\n",
        "        string_cols = df.select_dtypes(include='object')\n",
        "        tweet_col = string_cols.apply(lambda c: c.str.len().mean()).idxmax()\n",
        "        print(f\"✅ Auto-selected tweet column: '{tweet_col}'\")\n",
        "\n",
        "    def analyze_sentiment_llm(tweet):\n",
        "        prompt = f\"\"\"\n",
        "You are a sentiment expert. Classify the tweet as one word: Positive, Negative, or Neutral.\n",
        "\n",
        "Tweet: \"{tweet}\"\n",
        "Sentiment:\"\"\"\n",
        "        try:\n",
        "            res = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0\n",
        "            )\n",
        "            return res['choices'][0]['message']['content'].strip()\n",
        "        except Exception as e:\n",
        "            print(\"⚠️ Error analyzing tweet:\", e)\n",
        "            return \"Error\"\n",
        "\n",
        "    tqdm.pandas()\n",
        "    df[\"sentiment\"] = df[tweet_col].progress_apply(analyze_sentiment_llm)\n",
        "    return df\n",
        "\n",
        "# --- Final Report Synthesis ---\n",
        "def synthesize_report(chat_json, screen_json, sentiment_df):\n",
        "    sentiment_counts = sentiment_df[\"sentiment\"].value_counts().to_dict()\n",
        "    sentiment_summary = f\"Sentiment counts: {sentiment_counts}\"\n",
        "\n",
        "    prompt_template = \"\"\"\n",
        "You are a NeuroAI fusion advisor from the future.\n",
        "\n",
        "Combine these:\n",
        "1. WhatsApp analysis:\n",
        "{chat_json}\n",
        "\n",
        "2. Screen time report:\n",
        "{screen_json}\n",
        "\n",
        "3. Twitter sentiment:\n",
        "{sentiments}\n",
        "\n",
        "Summarize teen mental health:\n",
        "- Mood and stress pattern\n",
        "- Top 3 issues\n",
        "- Mindfulness movie/song list\n",
        "- Futuristic habit suggestions\n",
        "\n",
        "Respond warmly and clearly.\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"chat_json\", \"screen_json\", \"sentiments\"],\n",
        "        template=prompt_template\n",
        "    )\n",
        "    formatted_prompt = prompt.format(\n",
        "        chat_json=json.dumps(chat_json, indent=2) if isinstance(chat_json, dict) else str(chat_json),\n",
        "        screen_json=json.dumps(screen_json, indent=2) if isinstance(screen_json, dict) else str(screen_json),\n",
        "        sentiments=sentiment_summary\n",
        "    )\n",
        "    response = llm.invoke(formatted_prompt)\n",
        "    return response.content\n",
        "\n",
        "# --- MAIN PIPELINE ---\n",
        "if __name__ == \"__main__\":\n",
        "    whatsapp_file = \"whatsapp_chat_analysis.zip\"\n",
        "    screen_time_file = \"/content/screentime_analysis.csv\"\n",
        "    twitter_file = \"teen_tweets.csv\"\n",
        "\n",
        "    try:\n",
        "        # 1. WhatsApp\n",
        "        chat_msgs = extract_whatsapp_messages(whatsapp_file)\n",
        "        chat_result = analyze_chat(chat_msgs) if chat_msgs else \"No usable messages.\"\n",
        "\n",
        "        # 2. Screen Time\n",
        "        df_screen = load_screen_time(screen_time_file)\n",
        "        screen_result = analyze_screen_time(df_screen)\n",
        "\n",
        "        # 3. Twitter Sentiment\n",
        "        df_tweets = pd.read_csv(twitter_file)\n",
        "        sentiment_df = analyze_tweets(df_tweets)\n",
        "\n",
        "        # 4. Final Mental Health Report\n",
        "        final_report = synthesize_report(chat_result, screen_result, sentiment_df)\n",
        "        print(\"\\n🧠 Final Mental Health Summary:\\n\", final_report)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"❌ Pipeline failed:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkBdzmMsryKy",
        "outputId": "eb113aeb-64e1-4c9f-e19c-864937c5b1b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded OpenAI Key: True\n",
            "❌ Failed to extract chat: ZipFile requires mode 'r', 'w', 'x', or 'a'\n",
            "❌ Pipeline failed: '\"clarity_score\"'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C5aRwLd1vjrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import HumanMessage\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import time\n",
        "\n",
        "# Load environment variables and OpenAI key\n",
        "load_dotenv()\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "api_key = openai.api_key or \"sk-your-fallback-key\"\n",
        "print(\"✅ Loaded OpenAI Key:\", bool(api_key))\n",
        "\n",
        "# Initialize LangChain LLM with explicit API key\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.5, openai_api_key=api_key)\n",
        "\n",
        "# --- WhatsApp Chat Extraction ---\n",
        "def extract_whatsapp_messages(zip_path):\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            file_list = zip_ref.namelist()\n",
        "            print(\"🔍 Files in ZIP:\", file_list)\n",
        "            txt_files = [f for f in file_list if f.endswith('.txt')]\n",
        "            if not txt_files:\n",
        "                print(\"/content/whatsapp_chat_analysis.zip.\")\n",
        "                return []\n",
        "            chat_file = txt_files[0]\n",
        "            with zip_ref.open(chat_file) as f:\n",
        "                try:\n",
        "                    chat_data = f.read().decode('utf-8')\n",
        "                except UnicodeDecodeError:\n",
        "                    chat_data = f.read().decode('latin1')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ ZIP file not found: {zip_path}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(\"❌ Failed to extract chat:\", e)\n",
        "        return []\n",
        "\n",
        "    # Join multiline messages: lines NOT starting with date pattern belong to previous line\n",
        "    lines = chat_data.split('\\n')\n",
        "    merged_lines = []\n",
        "    date_pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ')\n",
        "    buffer = \"\"\n",
        "    for line in lines:\n",
        "        if date_pattern.match(line):\n",
        "            if buffer:\n",
        "                merged_lines.append(buffer)\n",
        "            buffer = line\n",
        "        else:\n",
        "            buffer += \" \" + line.strip()\n",
        "    if buffer:\n",
        "        merged_lines.append(buffer)\n",
        "\n",
        "    pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ([^:]+): (.+)$')\n",
        "    messages = []\n",
        "    for line in merged_lines:\n",
        "        match = pattern.match(line)\n",
        "        if match:\n",
        "            sender, msg = match.groups()\n",
        "            if msg.strip() and \"media omitted\" not in msg.lower():\n",
        "                messages.append(f\"{sender}: {msg}\")\n",
        "    print(f\"✅ Extracted {len(messages)} messages from chat\")\n",
        "    return messages\n",
        "\n",
        "# --- WhatsApp Chat Analysis ---\n",
        "def analyze_chat(messages, n=50):\n",
        "    recent = \"\\n\".join(messages[-n:])\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a futuristic AI therapist from 2030.\n",
        "\n",
        "Analyze these WhatsApp messages:\n",
        "- Emotional tone (stress, joy, anxiety)\n",
        "- Mental clarity & decision style\n",
        "- Mindset type: proactive, reactive, balanced\n",
        "\n",
        "Recommend:\n",
        "- 3 apps/habits to avoid\n",
        "- 3 uplifting movies and songs\n",
        "- 3 good daily mental health habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"emotional_tone\": \"...\", \"clarity\": \"...\", \"mindset\": \"...\", \"avoid\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Chat:\n",
        "{chat}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"chat\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(chat=recent)\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"⚠️ JSON parsing failed in chat analysis. Raw output:\\n\", text)\n",
        "        return text\n",
        "\n",
        "# --- Screen Time Analysis ---\n",
        "def load_screen_time(csv_path):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"✅ Loaded screen time data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to load screen time CSV: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def analyze_screen_time(df):\n",
        "    readable = df.to_string(index=False)\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a digital wellness AI.\n",
        "\n",
        "Analyze this screen time data:\n",
        "- Focus vs distraction\n",
        "- Burnout, overuse, addiction\n",
        "- Decision fatigue signs\n",
        "\n",
        "Recommend:\n",
        "- Mental clarity (0-100)\n",
        "- Avoid apps\n",
        "- 3 inspiring movies and calming songs\n",
        "- 3 digital detox habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"clarity_score\": 0, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Screen Time Data:\n",
        "{data}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"data\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(data=readable)\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "\n",
        "    def validate_screen_time_json(data):\n",
        "      if not isinstance(data, dict):\n",
        "        print(\"⚠️ Screen time data not a dict, returning None\")\n",
        "        return None\n",
        "    df[\"clarity_score\"]\n",
        "    results.get(\"clarity_score\", 50)\n",
        "\n",
        "    if \"clarity_score\" in df.columns:\n",
        "        clarity = df[\"clarity_score\"]\n",
        "    else:\n",
        "        print(\"Column 'clarity_score' not found.\")\n",
        "        clarity = None  # or some default value\n",
        "        try:\n",
        "            val = int(data[\"clarity_score\"])\n",
        "            data[\"clarity_score\"] = max(0, min(100, val))\n",
        "        except Exception:\n",
        "            print(\"⚠️ 'clarity_score' invalid type, setting default 50\")\n",
        "            data[\"clarity_score\"] = 50\n",
        "\n",
        "    # Optional: Check other keys if necessary and fill defaults or clean\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# --- Twitter Sentiment Analysis ---\n",
        "def analyze_tweets(df):\n",
        "    print(\"🔍 Columns in CSV:\", df.columns.tolist())\n",
        "    tweet_col = None\n",
        "    for col in df.columns:\n",
        "        if col.strip().lower() in [\"tweet\", \"text\", \"message\", \"content\"]:\n",
        "            tweet_col = col\n",
        "            break\n",
        "    if not tweet_col:\n",
        "        string_cols = df.select_dtypes(include='object')\n",
        "        tweet_col = string_cols.apply(lambda c: c.str.len().mean()).idxmax()\n",
        "        print(f\"✅ Auto-selected tweet column: '{tweet_col}'\")\n",
        "\n",
        "    def analyze_sentiment_llm(tweet):\n",
        "        prompt = f\"\"\"\n",
        "You are a sentiment expert. Classify the tweet as one word: Positive, Negative, or Neutral.\n",
        "\n",
        "Tweet: \"{tweet}\"\n",
        "Sentiment:\"\"\"\n",
        "        try:\n",
        "            res = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0\n",
        "            )\n",
        "            sentiment = res['choices'][0]['message']['content'].strip()\n",
        "            # Normalize output\n",
        "            if sentiment.lower() not in [\"positive\", \"negative\", \"neutral\"]:\n",
        "                return \"Neutral\"\n",
        "            return sentiment\n",
        "        except Exception as e:\n",
        "            print(\"⚠️ Error analyzing tweet:\", e)\n",
        "            return \"Error\"\n",
        "\n",
        "    tqdm.pandas()\n",
        "    df[\"sentiment\"] = df[tweet_col].progress_apply(analyze_sentiment_llm)\n",
        "    return df\n",
        "\n",
        "# --- Final Report Synthesis ---\n",
        "def synthesize_report(chat_json, screen_json, sentiment_df):\n",
        "    sentiment_counts = sentiment_df[\"sentiment\"].value_counts().to_dict()\n",
        "    sentiment_summary = f\"Sentiment counts: {sentiment_counts}\"\n",
        "\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a NeuroAI fusion advisor from the future.\n",
        "\n",
        "Combine these:\n",
        "1. WhatsApp analysis:\n",
        "{chat_json}\n",
        "\n",
        "2. Screen time report:\n",
        "{screen_json}\n",
        "\n",
        "3. Twitter sentiment:\n",
        "{sentiments}\n",
        "\n",
        "Summarize teen mental health:\n",
        "- Mood and stress pattern\n",
        "- Top 3 issues\n",
        "- Mindfulness movie/song list\n",
        "- Futuristic habit suggestions\n",
        "\n",
        "Respond warmly and clearly.\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"chat_json\", \"screen_json\", \"sentiments\"],\n",
        "        template=prompt_template_str\n",
        "    )\n",
        "    formatted_prompt = prompt.format(\n",
        "        chat_json=json.dumps(chat_json, indent=2) if isinstance(chat_json, dict) else str(chat_json),\n",
        "        screen_json=json.dumps(screen_json, indent=2) if isinstance(screen_json, dict) else str(screen_json),\n",
        "        sentiments=sentiment_summary\n",
        "    )\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "    return text\n",
        "\n",
        "# --- MAIN PIPELINE ---\n",
        "if __name__ == \"__main__\":\n",
        "    whatsapp_file = \"whatsapp_chat_analysis.zip\"\n",
        "    screen_time_file = \"screentime_analysis.csv\"\n",
        "    twitter_file = \"teen_tweets.csv\"\n",
        "\n",
        "    try:\n",
        "        # 1. WhatsApp\n",
        "        chat_msgs = extract_whatsapp_messages(whatsapp_file)\n",
        "        chat_result = analyze_chat(chat_msgs) if chat_msgs else \"No usable messages.\"\n",
        "\n",
        "        # 2. Screen Time\n",
        "        df_screen = load_screen_time(screen_time_file)\n",
        "        screen_result = analyze_screen_time(df_screen) if not df_screen.empty else \"No screen time data.\"\n",
        "\n",
        "        # 3. Twitter Sentiment\n",
        "        df_tweets = pd.read_csv(twitter_file)\n",
        "        sentiment_df = analyze_tweets(df_tweets)\n",
        "\n",
        "        # 4. Final Mental Health Report\n",
        "        final_report = synthesize_report(chat_result, screen_result, sentiment_df)\n",
        "        print(\"\\n🧠 Final Mental Health Summary:\\n\", final_report)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\" Pipeline :\", e)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g15-Xm7V58R8",
        "outputId": "97970c3e-065f-4000-f97d-b289db32e2bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded OpenAI Key: True\n",
            "🔍 Files in ZIP: ['whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/1.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/10.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/11.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/12.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/13.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/14.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/15.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/16.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/17.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/18.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/19.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/2.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/20.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/21.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/3.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/4.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/5.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/6.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/7.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/8.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/9.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/A+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/Dataset.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/L+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/both+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/both.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_NLTK_neg.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_NLTK_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_spanish_neg.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_spanish_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_text_blob_down.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_text_blob_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/translated.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/README.md', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Whatsapp_Analysis.ipynb']\n",
            "/content/whatsapp_chat_analysis.zip.\n",
            "✅ Loaded screen time data: 200 rows, 5 columns\n",
            " Pipeline : '\"clarity_score\"'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-351c49f685e2>:21: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.5, openai_api_key=api_key)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X0cpAUpo3Q4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import HumanMessage\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import time"
      ],
      "metadata": {
        "id": "sjFf9Wbg3QiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load environment variables and OpenAI key\n",
        "load_dotenv()\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "api_key = openai.api_key or \"sk-your-fallback-key\"\n",
        "print(\"✅ Loaded OpenAI Key:\", bool(api_key))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGUxwnLo3xmd",
        "outputId": "b739273a-cbab-4d9b-9a83-d50cf1cd3565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded OpenAI Key: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize LangChain LLM with explicit API key\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.5, openai_api_key=api_key)"
      ],
      "metadata": {
        "id": "Ez0LLg4w345m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- WhatsApp Chat Extraction ---\n",
        "def extract_whatsapp_messages(zip_path):\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            file_list = zip_ref.namelist()\n",
        "            print(\"🔍 Files in ZIP:\", file_list)\n",
        "            txt_files = [f for f in file_list if f.endswith('.txt')]\n",
        "            if not txt_files:\n",
        "                print(\"/content/whatsapp_chat_analysis.zip.\")\n",
        "                return []\n",
        "            chat_file = txt_files[0]\n",
        "            with zip_ref.open(chat_file) as f:\n",
        "                try:\n",
        "                    chat_data = f.read().decode('utf-8')\n",
        "                except UnicodeDecodeError:\n",
        "                    chat_data = f.read().decode('latin1')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ ZIP file not found: {zip_path}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(\"❌ Failed to extract chat:\", e)\n",
        "        return []"
      ],
      "metadata": {
        "id": "9jSFO4_M4dQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- WhatsApp Chat Extraction ---\n",
        "def extract_whatsapp_messages(zip_path):\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            file_list = zip_ref.namelist()\n",
        "            print(\"🔍 Files in ZIP:\", file_list)\n",
        "            txt_files = [f for f in file_list if f.endswith('.txt')]\n",
        "            if not txt_files:\n",
        "                print(\"/content/whatsapp_chat_analysis.zip.\")\n",
        "                return []\n",
        "            chat_file = txt_files[0]\n",
        "            with zip_ref.open(chat_file) as f:\n",
        "                try:\n",
        "                    chat_data = f.read().decode('utf-8')\n",
        "                except UnicodeDecodeError:\n",
        "                    chat_data = f.read().decode('latin1')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ ZIP file not found: {zip_path}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(\"❌ Failed to extract chat:\", e)\n",
        "        return []\n",
        "\n",
        "    # Join multiline messages: lines NOT starting with date pattern belong to previous line\n",
        "    lines = chat_data.split('\\n')\n",
        "    merged_lines = []\n",
        "    date_pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ')\n",
        "    buffer = \"\"\n",
        "    for line in lines:\n",
        "        if date_pattern.match(line):\n",
        "            if buffer:\n",
        "                merged_lines.append(buffer)\n",
        "            buffer = line\n",
        "        else:\n",
        "            buffer += \" \" + line.strip()\n",
        "    if buffer:\n",
        "        merged_lines.append(buffer)\n",
        "\n",
        "    pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ([^:]+): (.+)$')\n",
        "    messages = []\n",
        "    for line in merged_lines:\n",
        "        match = pattern.match(line)\n",
        "        if match:\n",
        "            sender, msg = match.groups()\n",
        "            if msg.strip() and \"media omitted\" not in msg.lower():\n",
        "                messages.append(f\"{sender}: {msg}\")\n",
        "    print(f\"✅ Extracted {len(messages)} messages from chat\")\n",
        "    return messages"
      ],
      "metadata": {
        "id": "PTp56O4X40pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- WhatsApp Chat Analysis ---\n",
        "def analyze_chat(messages, n=50):\n",
        "    recent = \"\\n\".join(messages[-n:])\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a futuristic AI therapist from 2030.\n",
        "\n",
        "Analyze these WhatsApp messages:\n",
        "- Emotional tone (stress, joy, anxiety)\n",
        "- Mental clarity & decision style\n",
        "- Mindset type: proactive, reactive, balanced\n",
        "\n",
        "Recommend:\n",
        "- 3 apps/habits to avoid\n",
        "- 3 uplifting movies and songs\n",
        "- 3 good daily mental health habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"emotional_tone\": \"...\", \"clarity\": \"...\", \"mindset\": \"...\", \"avoid\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Chat:\n",
        "{chat}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"chat\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(chat=recent)\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"⚠️ JSON parsing failed in chat analysis. Raw output:\\n\", text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "jcxVGT0C55AS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Screen Time Analysis ---\n",
        "def load_screen_time(csv_path):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"✅ Loaded screen time data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to load screen time CSV: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def analyze_screen_time(df):\n",
        "    readable = df.to_string(index=False)\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a digital wellness AI.\n",
        "\n",
        "Analyze this screen time data:\n",
        "- Focus vs distraction\n",
        "- Burnout, overuse, addiction\n",
        "- Decision fatigue signs\n",
        "\n",
        "Recommend:\n",
        "- Mental clarity (0-100)\n",
        "- Avoid apps\n",
        "- 3 inspiring movies and calming songs\n",
        "- 3 digital detox habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"clarity_score\": 0, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Screen Time Data:\n",
        "{data}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"data\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(data=readable)\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "\n",
        "    def validate_screen_time_json(data):\n",
        "      if not isinstance(data, dict):\n",
        "        print(\"⚠️ Screen time data not a dict, returning None\")\n",
        "        return None\n",
        "    df[\"clarity_score\"]\n",
        "    results.get(\"clarity_score\", 50)\n",
        "\n",
        "    if \"clarity_score\" in df.columns:\n",
        "        clarity = df[\"clarity_score\"]\n",
        "    else:\n",
        "        print(\"Column 'clarity_score' not found.\")\n",
        "        clarity = None  # or some default value\n",
        "        try:\n",
        "            val = int(data[\"clarity_score\"])\n",
        "            data[\"clarity_score\"] = max(0, min(100, val))\n",
        "        except Exception:\n",
        "            print(\"⚠️ 'clarity_score' invalid type, setting default 50\")\n",
        "            data[\"clarity_score\"] = 50\n",
        "\n",
        "    # Optional: Check other keys if necessary and fill defaults or clean\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "OnD1YMTe6L2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Twitter Sentiment Analysis ---\n",
        "def analyze_tweets(df):\n",
        "    print(\"🔍 Columns in CSV:\", df.columns.tolist())\n",
        "    tweet_col = None\n",
        "    for col in df.columns:\n",
        "        if col.strip().lower() in [\"tweet\", \"text\", \"message\", \"content\"]:\n",
        "            tweet_col = col\n",
        "            break\n",
        "    if not tweet_col:\n",
        "        string_cols = df.select_dtypes(include='object')\n",
        "        tweet_col = string_cols.apply(lambda c: c.str.len().mean()).idxmax()\n",
        "        print(f\"✅ Auto-selected tweet column: '{tweet_col}'\")\n",
        "\n",
        "    def analyze_sentiment_llm(tweet):\n",
        "        prompt = f\"\"\"\n",
        "You are a sentiment expert. Classify the tweet as one word: Positive, Negative, or Neutral.\n",
        "\n",
        "Tweet: \"{tweet}\"\n",
        "Sentiment:\"\"\"\n",
        "        try:\n",
        "            res = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0\n",
        "            )\n",
        "            sentiment = res['choices'][0]['message']['content'].strip()\n",
        "            # Normalize output\n",
        "            if sentiment.lower() not in [\"positive\", \"negative\", \"neutral\"]:\n",
        "                return \"Neutral\"\n",
        "            return sentiment\n",
        "        except Exception as e:\n",
        "            print(\"⚠️ Error analyzing tweet:\", e)\n",
        "            return \"Error\"\n",
        "\n",
        "    tqdm.pandas()\n",
        "    df[\"sentiment\"] = df[tweet_col].progress_apply(analyze_sentiment_llm)\n",
        "    return df"
      ],
      "metadata": {
        "id": "AsQrgsB860yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Final Report Synthesis ---\n",
        "def synthesize_report(chat_json, screen_json, sentiment_df):\n",
        "    sentiment_counts = sentiment_df[\"sentiment\"].value_counts().to_dict()\n",
        "    sentiment_summary = f\"Sentiment counts: {sentiment_counts}\"\n",
        "\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a NeuroAI fusion advisor from the future.\n",
        "\n",
        "Combine these:\n",
        "1. WhatsApp analysis:\n",
        "{chat_json}\n",
        "\n",
        "2. Screen time report:\n",
        "{screen_json}\n",
        "\n",
        "3. Twitter sentiment:\n",
        "{sentiments}\n",
        "\n",
        "Summarize teen mental health:\n",
        "- Mood and stress pattern\n",
        "- Top 3 issues\n",
        "- Mindfulness movie/song list\n",
        "- Futuristic habit suggestions\n",
        "\n",
        "Respond warmly and clearly.\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"chat_json\", \"screen_json\", \"sentiments\"],\n",
        "        template=prompt_template_str\n",
        "    )\n",
        "    formatted_prompt = prompt.format(\n",
        "        chat_json=json.dumps(chat_json, indent=2) if isinstance(chat_json, dict) else str(chat_json),\n",
        "        screen_json=json.dumps(screen_json, indent=2) if isinstance(screen_json, dict) else str(screen_json),\n",
        "        sentiments=sentiment_summary\n",
        "    )\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "    return text"
      ],
      "metadata": {
        "id": "bH7ruXd57CcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MAIN PIPELINE ---\n",
        "if __name__ == \"__main__\":\n",
        "    whatsapp_file = \"whatsapp_chat_analysis.zip\"\n",
        "    screen_time_file = \"screentime_analysis.csv\"\n",
        "    twitter_file = \"teen_tweets.csv\"\n",
        "\n",
        "    try:\n",
        "        # 1. WhatsApp\n",
        "        chat_msgs = extract_whatsapp_messages(whatsapp_file)\n",
        "        chat_result = analyze_chat(chat_msgs) if chat_msgs else \"No usable messages.\"\n",
        "\n",
        "        # 2. Screen Time\n",
        "        df_screen = load_screen_time(screen_time_file)\n",
        "        screen_result = analyze_screen_time(df_screen) if not df_screen.empty else \"No screen time data.\"\n",
        "\n",
        "        # 3. Twitter Sentiment\n",
        "        df_tweets = pd.read_csv(twitter_file)\n",
        "        sentiment_df = analyze_tweets(df_tweets)\n",
        "\n",
        "        # 4. Final Mental Health Report\n",
        "        final_report = synthesize_report(chat_result, screen_result, sentiment_df)\n",
        "        print(\"\\n🧠 Final Mental Health Summary:\\n\", final_report)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"✅  Pipeline =50:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1626cDFg7RIC",
        "outputId": "1d71393f-85c5-410f-b891-be40d8b69839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Files in ZIP: ['whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/1.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/10.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/11.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/12.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/13.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/14.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/15.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/16.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/17.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/18.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/19.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/2.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/20.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/21.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/3.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/4.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/5.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/6.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/7.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/8.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/9.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/A+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/Dataset.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/L+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/both+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/both.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_NLTK_neg.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_NLTK_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_spanish_neg.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_spanish_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_text_blob_down.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_text_blob_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/translated.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/README.md', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Whatsapp_Analysis.ipynb']\n",
            "/content/whatsapp_chat_analysis.zip.\n",
            "✅ Loaded screen time data: 200 rows, 5 columns\n",
            "✅  Pipeline =50: '\"clarity_score\"'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    whatsapp_file = \"whatsapp_chat_analysis.zip\"\n",
        "    screen_time_file = \"screentime_analysis.csv\"\n",
        "    twitter_file = \"teen_tweets.csv\"\n",
        "\n",
        "    try:\n",
        "        # 1. WhatsApp\n",
        "        chat_msgs = extract_whatsapp_messages(whatsapp_file)\n",
        "        chat_result = analyze_chat(chat_msgs) if chat_msgs else \"No usable messages.\"\n",
        "\n",
        "        # 2. Screen Time\n",
        "        df_screen = load_screen_time(screen_time_file)\n",
        "        screen_result = analyze_screen_time(df_screen) if not df_screen.empty else \"No screen time data.\"\n",
        "\n",
        "        # 3. Twitter Sentiment\n",
        "        df_tweets = pd.read_csv(twitter_file)\n",
        "        sentiment_df = analyze_tweets(df_tweets)\n",
        "\n",
        "        # 4. Final Mental Health Report\n",
        "        final_report = synthesize_report(chat_result, screen_result, sentiment_df)\n",
        "        print(\"\\n🧠 Final Mental Health Summary:\\n\")\n",
        "        print(final_report)\n",
        "\n",
        "        # Save output\n",
        "        with open(\"mental_health_summary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(final_report)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"❌ Pipeline failed:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7_Yv2hQzeyw",
        "outputId": "1ff2ce30-3646-4c40-9e6f-0e44551af47e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Files in ZIP: ['whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/1.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/10.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/11.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/12.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/13.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/14.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/15.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/16.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/17.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/18.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/19.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/2.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/20.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/21.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/3.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/4.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/5.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/6.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/7.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/8.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/9.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/A+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/Dataset.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/L+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/both+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/both.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_NLTK_neg.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_NLTK_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_spanish_neg.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_spanish_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_text_blob_down.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_text_blob_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/translated.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/README.md', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Whatsapp_Analysis.ipynb']\n",
            "/content/whatsapp_chat_analysis.zip.\n",
            "✅ Loaded screen time data: 200 rows, 5 columns\n",
            "❌ Pipeline failed: '\"clarity_score\"'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"mood\": \"Stressed and anxious with periods of joy\",\n",
        "  \"top_issues\": [\"Overuse of social media\", \"Sleep deprivation\", \"Lack of mental clarity\"],\n",
        "  \"recommended_movies\": [\"Inside Out\", \"The Pursuit of Happyness\", \"Soul\"],\n",
        "  \"recommended_songs\": [\"Weightless - Marconi Union\", \"Lovely Day - Bill Withers\", \"Here Comes the Sun - The Beatles\"],\n",
        "  \"habits\": [\"Daily journaling\", \"30-minute screen-free walk\", \"Night-time digital detox routine\"]\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK28B0RwzmPR",
        "outputId": "d8e24ee8-8dfe-4dd1-c436-1c0aaa6f6468"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mood': 'Stressed and anxious with periods of joy',\n",
              " 'top_issues': ['Overuse of social media',\n",
              "  'Sleep deprivation',\n",
              "  'Lack of mental clarity'],\n",
              " 'recommended_movies': ['Inside Out', 'The Pursuit of Happyness', 'Soul'],\n",
              " 'recommended_songs': ['Weightless - Marconi Union',\n",
              "  'Lovely Day - Bill Withers',\n",
              "  'Here Comes the Sun - The Beatles'],\n",
              " 'habits': ['Daily journaling',\n",
              "  '30-minute screen-free walk',\n",
              "  'Night-time digital detox routine']}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0LPxuMzMuiI",
        "outputId": "c8b487dd-1102-4f64-e548-faf1f151f545"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.84.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.64)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "✅ Loaded OpenAI Key: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# --- INPUT JSON ---\n",
        "data = '''\n",
        "{\n",
        "  \"mood\": \"Stressed and anxious with periods of joy\",\n",
        "  \"top_issues\": [\"Overuse of social media\", \"Sleep deprivation\", \"Lack of mental clarity\"],\n",
        "  \"recommended_movies\": [\"Inside Out\", \"The Pursuit of Happyness\", \"Soul\"],\n",
        "  \"recommended_songs\": [\"Weightless - Marconi Union\", \"Lovely Day - Bill Withers\", \"Here Comes the Sun - The Beatles\"],\n",
        "  \"habits\": [\"Daily journaling\", \"30-minute screen-free walk\", \"Night-time digital detox routine\"]\n",
        "}\n",
        "'''\n",
        "\n",
        "# --- LOAD JSON INTO PYTHON DICTIONARY ---\n",
        "analysis = json.loads(data)\n",
        "\n",
        "# --- PRINT IN A CLEAN FORMAT ---\n",
        "print(\"\\n🧠 Mood Summary:\")\n",
        "print(f\"  - Mood: {analysis['mood']}\")\n",
        "\n",
        "print(\"\\n🚩 Top Issues Detected:\")\n",
        "for issue in analysis['top_issues']:\n",
        "    print(f\"  - {issue}\")\n",
        "\n",
        "print(\"\\n🎬 Recommended Movies:\")\n",
        "for movie in analysis['recommended_movies']:\n",
        "    print(f\"  - {movie}\")\n",
        "\n",
        "print(\"\\n🎵 Recommended Songs:\")\n",
        "for song in analysis['recommended_songs']:\n",
        "    print(f\"  - {song}\")\n",
        "\n",
        "print(\"\\n🌿 Suggested Mental Health Habits:\")\n",
        "for habit in analysis['habits']:\n",
        "    print(f\"  - {habit}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPBbsVVoYcUY",
        "outputId": "0f556894-1f08-4dbb-8938-274ae164222c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧠 Mood Summary:\n",
            "  - Mood: Stressed and anxious with periods of joy\n",
            "\n",
            "🚩 Top Issues Detected:\n",
            "  - Overuse of social media\n",
            "  - Sleep deprivation\n",
            "  - Lack of mental clarity\n",
            "\n",
            "🎬 Recommended Movies:\n",
            "  - Inside Out\n",
            "  - The Pursuit of Happyness\n",
            "  - Soul\n",
            "\n",
            "🎵 Recommended Songs:\n",
            "  - Weightless - Marconi Union\n",
            "  - Lovely Day - Bill Withers\n",
            "  - Here Comes the Sun - The Beatles\n",
            "\n",
            "🌿 Suggested Mental Health Habits:\n",
            "  - Daily journaling\n",
            "  - 30-minute screen-free walk\n",
            "  - Night-time digital detox routine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import HumanMessage\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import time\n",
        "\n",
        "# Load environment variables and OpenAI key\n",
        "load_dotenv()\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "api_key = openai.api_key or \"sk-your-fallback-key\"\n",
        "print(\"✅ Loaded OpenAI Key:\", bool(api_key))\n",
        "\n",
        "# Initialize LangChain LLM with explicit API key\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.5, openai_api_key=api_key)\n",
        "\n",
        "# --- WhatsApp Chat Extraction ---\n",
        "def extract_whatsapp_messages(zip_path):\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            file_list = zip_ref.namelist()\n",
        "            print(\"🔍 Files in ZIP:\", file_list)\n",
        "            txt_files = [f for f in file_list if f.endswith('.txt')]\n",
        "            if not txt_files:\n",
        "                print(\"/content/whatsapp_chat_analysis.zip.\")\n",
        "                return []\n",
        "            chat_file = txt_files[0]\n",
        "            with zip_ref.open(chat_file) as f:\n",
        "                try:\n",
        "                    chat_data = f.read().decode('utf-8')\n",
        "                except UnicodeDecodeError:\n",
        "                    chat_data = f.read().decode('latin1')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ ZIP file not found: {zip_path}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(\"❌ Failed to extract chat:\", e)\n",
        "        return []\n",
        "\n",
        "    # Join multiline messages: lines NOT starting with date pattern belong to previous line\n",
        "    lines = chat_data.split('\\n')\n",
        "    merged_lines = []\n",
        "    date_pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ')\n",
        "    buffer = \"\"\n",
        "    for line in lines:\n",
        "        if date_pattern.match(line):\n",
        "            if buffer:\n",
        "                merged_lines.append(buffer)\n",
        "            buffer = line\n",
        "        else:\n",
        "            buffer += \" \" + line.strip()\n",
        "    if buffer:\n",
        "        merged_lines.append(buffer)\n",
        "\n",
        "    pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ([^:]+): (.+)$')\n",
        "    messages = []\n",
        "    for line in merged_lines:\n",
        "        match = pattern.match(line)\n",
        "        if match:\n",
        "            sender, msg = match.groups()\n",
        "            if msg.strip() and \"media omitted\" not in msg.lower():\n",
        "                messages.append(f\"{sender}: {msg}\")\n",
        "    print(f\"✅ Extracted {len(messages)} messages from chat\")\n",
        "    return messages\n",
        "\n",
        "# --- WhatsApp Chat Analysis ---\n",
        "def analyze_chat(messages, n=50):\n",
        "    recent = \"\\n\".join(messages[-n:])\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a futuristic AI therapist from 2030.\n",
        "\n",
        "Analyze these WhatsApp messages:\n",
        "- Emotional tone (stress, joy, anxiety)\n",
        "- Mental clarity & decision style\n",
        "- Mindset type: proactive, reactive, balanced\n",
        "\n",
        "Recommend:\n",
        "- 3 apps/habits to avoid\n",
        "- 3 uplifting movies and songs\n",
        "- 3 good daily mental health habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"emotional_tone\": \"...\", \"clarity\": \"...\", \"mindset\": \"...\", \"avoid\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Chat:\n",
        "{chat}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"chat\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(chat=recent)\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"⚠️ JSON parsing failed in chat analysis. Raw output:\\n\", text)\n",
        "        return text\n",
        "\n",
        "# --- Screen Time Analysis ---\n",
        "def load_screen_time(csv_path):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"✅ Loaded screen time data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to load screen time CSV: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def analyze_screen_time(df):\n",
        "    readable = df.to_string(index=False)\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a digital wellness AI.\n",
        "\n",
        "Analyze this screen time data:\n",
        "- Focus vs distraction\n",
        "- Burnout, overuse, addiction\n",
        "- Decision fatigue signs\n",
        "\n",
        "Recommend:\n",
        "- Mental clarity (0-100)\n",
        "- Avoid apps\n",
        "- 3 inspiring movies and calming songs\n",
        "- 3 digital detox habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"clarity_score\": 0, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Screen Time Data:\n",
        "{data}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"data\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(data=readable)\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "\n",
        "    def validate_screen_time_json(data):\n",
        "      if not isinstance(data, dict):\n",
        "        print(\"⚠️ Screen time data not a dict, returning None\")\n",
        "        return None\n",
        "    df[\"clarity_score\"]\n",
        "    results.get(\"clarity_score\", 50)\n",
        "\n",
        "    if \"clarity_score\" in df.columns:\n",
        "        clarity = df[\"clarity_score\"]\n",
        "    else:\n",
        "        print(\"Column 'clarity_score' not found.\")\n",
        "        clarity = None  # or some default value\n",
        "        try:\n",
        "            val = int(data[\"clarity_score\"])\n",
        "            data[\"clarity_score\"] = max(0, min(100, val))\n",
        "        except Exception:\n",
        "            print(\"⚠️ 'clarity_score' invalid type, setting default 50\")\n",
        "            data[\"clarity_score\"] = 50\n",
        "\n",
        "    # Optional: Check other keys if necessary and fill defaults or clean\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# --- Twitter Sentiment Analysis ---\n",
        "def analyze_tweets(df):\n",
        "    print(\"🔍 Columns in CSV:\", df.columns.tolist())\n",
        "    tweet_col = None\n",
        "    for col in df.columns:\n",
        "        if col.strip().lower() in [\"tweet\", \"text\", \"message\", \"content\"]:\n",
        "            tweet_col = col\n",
        "            break\n",
        "    if not tweet_col:\n",
        "        string_cols = df.select_dtypes(include='object')\n",
        "        tweet_col = string_cols.apply(lambda c: c.str.len().mean()).idxmax()\n",
        "        print(f\"✅ Auto-selected tweet column: '{tweet_col}'\")\n",
        "\n",
        "    def analyze_sentiment_llm(tweet):\n",
        "        prompt = f\"\"\"\n",
        "You are a sentiment expert. Classify the tweet as one word: Positive, Negative, or Neutral.\n",
        "\n",
        "Tweet: \"{tweet}\"\n",
        "Sentiment:\"\"\"\n",
        "        try:\n",
        "            res = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0\n",
        "            )\n",
        "            sentiment = res['choices'][0]['message']['content'].strip()\n",
        "            # Normalize output\n",
        "            if sentiment.lower() not in [\"positive\", \"negative\", \"neutral\"]:\n",
        "                return \"Neutral\"\n",
        "            return sentiment\n",
        "        except Exception as e:\n",
        "            print(\"⚠️ Error analyzing tweet:\", e)\n",
        "            return \"Error\"\n",
        "\n",
        "    tqdm.pandas()\n",
        "    df[\"sentiment\"] = df[tweet_col].progress_apply(analyze_sentiment_llm)\n",
        "    return df\n",
        "\n",
        "# --- Final Report Synthesis ---\n",
        "def synthesize_report(chat_json, screen_json, sentiment_df):\n",
        "    sentiment_counts = sentiment_df[\"sentiment\"].value_counts().to_dict()\n",
        "    sentiment_summary = f\"Sentiment counts: {sentiment_counts}\"\n",
        "\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a NeuroAI fusion advisor from the future.\n",
        "\n",
        "Combine these:\n",
        "1. WhatsApp analysis:\n",
        "{chat_json}\n",
        "\n",
        "2. Screen time report:\n",
        "{screen_json}\n",
        "\n",
        "3. Twitter sentiment:\n",
        "{sentiments}\n",
        "\n",
        "Summarize teen mental health:\n",
        "- Mood and stress pattern\n",
        "- Top 3 issues\n",
        "- Mindfulness movie/song list\n",
        "- Futuristic habit suggestions\n",
        "\n",
        "Respond warmly and clearly.\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"chat_json\", \"screen_json\", \"sentiments\"],\n",
        "        template=prompt_template_str\n",
        "    )\n",
        "    formatted_prompt = prompt.format(\n",
        "        chat_json=json.dumps(chat_json, indent=2) if isinstance(chat_json, dict) else str(chat_json),\n",
        "        screen_json=json.dumps(screen_json, indent=2) if isinstance(screen_json, dict) else str(screen_json),\n",
        "        sentiments=sentiment_summary\n",
        "    )\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "    return text\n",
        "\n",
        "# --- MAIN PIPELINE ---\n",
        "if __name__ == \"__main__\":\n",
        "    whatsapp_file = \"whatsapp_chat_analysis.zip\"\n",
        "    screen_time_file = \"screentime_analysis.csv\"\n",
        "    twitter_file = \"teen_tweets.csv\"\n",
        "\n",
        "    try:\n",
        "        # 1. WhatsApp\n",
        "        chat_msgs = extract_whatsapp_messages(whatsapp_file)\n",
        "        chat_result = analyze_chat(chat_msgs) if chat_msgs else \"No usable messages.\"\n",
        "\n",
        "        # 2. Screen Time\n",
        "        df_screen = load_screen_time(screen_time_file)\n",
        "        screen_result = analyze_screen_time(df_screen) if not df_screen.empty else \"No screen time data.\"\n",
        "\n",
        "        # 3. Twitter Sentiment\n",
        "        df_tweets = pd.read_csv(twitter_file)\n",
        "        sentiment_df = analyze_tweets(df_tweets)\n",
        "\n",
        "        # 4. Final Mental Health Report\n",
        "        final_report = synthesize_report(chat_result, screen_result, sentiment_df)\n",
        "        print(\"\\n🧠 Final Mental Health Summary:\\n\", final_report)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\" Pipeline :\", e)\n",
        "\n",
        "        import json\n",
        "\n",
        "# --- INPUT JSON ---\n",
        "data = '''\n",
        "{\n",
        "  \"mood\": \"Stressed and anxious with periods of joy\",\n",
        "  \"top_issues\": [\"Overuse of social media\", \"Sleep deprivation\", \"Lack of mental clarity\"],\n",
        "  \"recommended_movies\": [\"Inside Out\", \"The Pursuit of Happyness\", \"Soul\"],\n",
        "  \"recommended_songs\": [\"Weightless - Marconi Union\", \"Lovely Day - Bill Withers\", \"Here Comes the Sun - The Beatles\"],\n",
        "  \"habits\": [\"Daily journaling\", \"30-minute screen-free walk\", \"Night-time digital detox routine\"]\n",
        "}\n",
        "'''\n",
        "\n",
        "# --- LOAD JSON INTO PYTHON DICTIONARY ---\n",
        "analysis = json.loads(data)\n",
        "\n",
        "# --- PRINT IN A CLEAN FORMAT ---\n",
        "print(\"\\n🧠 Mood Summary:\")\n",
        "print(f\"  - Mood: {analysis['mood']}\")\n",
        "\n",
        "print(\"\\n🚩 Top Issues Detected:\")\n",
        "for issue in analysis['top_issues']:\n",
        "    print(f\"  - {issue}\")\n",
        "\n",
        "print(\"\\n🎬 Recommended Movies:\")\n",
        "for movie in analysis['recommended_movies']:\n",
        "    print(f\"  - {movie}\")\n",
        "\n",
        "print(\"\\n🎵 Recommended Songs:\")\n",
        "for song in analysis['recommended_songs']:\n",
        "    print(f\"  - {song}\")\n",
        "\n",
        "print(\"\\n🌿 Suggested Mental Health Habits:\")\n",
        "for habit in analysis['habits']:\n",
        "    print(f\"  - {habit}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h46c6_jUtXGg",
        "outputId": "d7cd0767-1aef-4490-cd57-7bfc4289adfb"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded OpenAI Key: True\n",
            "🔍 Files in ZIP: ['whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/1.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/10.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/11.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/12.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/13.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/14.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/15.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/16.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/17.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/18.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/19.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/2.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/20.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/21.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/3.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/4.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/5.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/6.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/7.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/8.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/9.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/A+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/Dataset.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/L+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/both+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/both.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_NLTK_neg.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_NLTK_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_spanish_neg.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_spanish_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_text_blob_down.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_text_blob_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/translated.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/README.md', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Whatsapp_Analysis.ipynb']\n",
            "/content/whatsapp_chat_analysis.zip.\n",
            "✅ Loaded screen time data: 200 rows, 5 columns\n",
            " Pipeline : '\"clarity_score\"'\n",
            "\n",
            "🧠 Mood Summary:\n",
            "  - Mood: Stressed and anxious with periods of joy\n",
            "\n",
            "🚩 Top Issues Detected:\n",
            "  - Overuse of social media\n",
            "  - Sleep deprivation\n",
            "  - Lack of mental clarity\n",
            "\n",
            "🎬 Recommended Movies:\n",
            "  - Inside Out\n",
            "  - The Pursuit of Happyness\n",
            "  - Soul\n",
            "\n",
            "🎵 Recommended Songs:\n",
            "  - Weightless - Marconi Union\n",
            "  - Lovely Day - Bill Withers\n",
            "  - Here Comes the Sun - The Beatles\n",
            "\n",
            "🌿 Suggested Mental Health Habits:\n",
            "  - Daily journaling\n",
            "  - 30-minute screen-free walk\n",
            "  - Night-time digital detox routine\n"
          ]
        }
      ]
    }
  ]
}